{
  "methods": [
    {
      "name": "Integrated Gradients (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Gradient-based attribution method that computes feature importance as the path integral of gradients from a baseline input to the actual input.",
        "main_functionalities": [
          "Compute token-/feature-level attributions relative to a baseline",
          "Support attributions to inputs and (in some implementations) intermediate layers"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Axiomatic Attribution for Deep Networks",
          "type": "paper",
          "year": 2017,
          "source": "PMLR (ICML 2017)",
          "url": "https://proceedings.mlr.press/v70/sundararajan17a.html",
          "note": "Introduces Integrated Gradients and its axiomatic framing."
        }
      ]
    },
    {
      "name": "Layer Integrated Gradients (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Integrated-Gradients-style attribution applied to internal layers to attribute importance to intermediate representations rather than inputs.",
        "main_functionalities": [
          "Compute attributions for intermediate layer inputs/outputs",
          "Enable layer-wise analysis beyond raw token saliency"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Saliency (Gradients) (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token-/feature-level saliency computed directly from gradients of the target output with respect to the input.",
        "main_functionalities": [
          "Compute gradient-based importance scores for each input feature/token"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
          "type": "paper",
          "year": 2013,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1312.6034",
          "note": "Early saliency-map formulation based on input gradients."
        }
      ]
    },
    {
      "name": "Input×Gradient (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Attribution method that multiplies the input features (often embeddings) by the gradient signal, yielding signed token contributions in embedding space.",
        "main_functionalities": [
          "Compute signed contributions by combining gradients with input magnitudes"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "DeepLIFT (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Backpropagation-based attribution method that compares activations to a reference (baseline) and propagates contribution scores.",
        "main_functionalities": [
          "Compute attributions relative to a reference input",
          "Propagate contribution scores through the network"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Learning Important Features Through Propagating Activation Differences",
          "type": "paper",
          "year": 2017,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1704.02685",
          "note": "Introduces DeepLIFT."
        }
      ]
    },
    {
      "name": "GradientSHAP (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Gradient-based Shapley-style attribution variant (as implemented in Captum) combining ideas from SHAP baselines and gradient integration/approximation.",
        "main_functionalities": [
          "Compute attributions using gradients with randomized baselines / sampling (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html",
          "note": "Introduces SHAP as a unified framework; gradient-based variants appear in later tool implementations."
        }
      ]
    },
    {
      "name": "Feature Ablation (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation-based attribution method that measures output change when features/tokens are replaced by a baseline value.",
        "main_functionalities": [
          "Estimate importance by ablation (feature removal/replacement) and observing output differences"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Occlusion (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation method that occludes parts of the input (e.g., tokens/spans) and measures the effect on the model output.",
        "main_functionalities": [
          "Compute importance by systematically occluding input segments and measuring output change"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Visualizing and Understanding Convolutional Networks",
          "type": "paper",
          "year": 2014,
          "source": "ECCV (Springer)",
          "url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53",
          "note": "Popularizes occlusion-style sensitivity analyses for interpreting neural models."
        }
      ]
    },
    {
      "name": "Kernel SHAP (InterpretML: ShapKernel)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Model-agnostic Shapley-value estimation using a Kernel SHAP-style procedure exposed in InterpretML as ShapKernel.",
        "main_functionalities": [
          "Compute local feature attributions for black-box predictors",
          "Aggregate attributions over a dataset for global summaries (workflow-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html",
          "note": "Introduces SHAP."
        }
      ]
    },
    {
      "name": "LIME (InterpretML: LimeTabular)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Local surrogate explanation method that fits an interpretable model around a specific instance using perturbed samples (InterpretML exposes a tabular variant as LimeTabular).",
        "main_functionalities": [
          "Generate local explanations by learning a simple surrogate model around an instance"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
          "type": "paper",
          "year": 2016,
          "source": "KDD",
          "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
          "note": "Introduces LIME."
        }
      ]
    },
    {
      "name": "Partial Dependence Plot (InterpretML: PartialDependence)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "global",
      "description": {
        "overview": "Global explanation that estimates the marginal effect of one (or more) features on the model prediction.",
        "main_functionalities": [
          "Compute marginal feature–prediction relationships over a dataset"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Morris Sensitivity Analysis (InterpretML: MorrisSensitivity)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "global",
      "description": {
        "overview": "Global sensitivity-analysis method that estimates how input features influence model outputs by sampling along trajectories in the input space.",
        "main_functionalities": [
          "Quantify feature influence via Morris-style sensitivity indices (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "SageMaker Clarify SHAP feature attributions",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Shapley-value-based feature attributions produced by SageMaker Clarify, documented as available for both specific predictions (local) and global model understanding; includes NLP explainability configuration for text features.",
        "main_functionalities": [
          "Compute Shapley-based feature attributions for predictions",
          "Provide dataset-level aggregation for global importance (as documented)",
          "Support NLP explainability with text-specific configuration"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud",
          "type": "paper",
          "year": 2021,
          "source": "Amazon Science (PDF)",
          "url": "https://assets.amazon.science/45/76/30bab4f14ccab96cfe8067ed2b4a/amazon-sagemaker-clarify-machine-learning-bias-detection-and-explainability-in-the-cloud.pdf",
          "note": "Describes Clarify’s SHAP-based explainability at scale."
        }
      ]
    },
    {
      "name": "SageMaker Clarify Partial Dependence Plots",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "global",
      "description": {
        "overview": "Partial Dependence Plots (PDP) generation in SageMaker Clarify to visualize marginal feature effects on model predictions.",
        "main_functionalities": [
          "Generate PDPs for selected features",
          "Support model-level explainability reports including PDP artifacts (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret Gradient attribution",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token-level feature attribution using input gradients (optionally multiplied by token embeddings) for transformer text classifiers.",
        "main_functionalities": [
          "Compute token attribution scores via gradients",
          "Return per-token signed relevance for a target class"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret Integrated Gradients attribution",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token-level Integrated Gradients attribution (optionally × token embeddings) for transformer text classifiers.",
        "main_functionalities": [
          "Compute IG-style token attributions relative to a baseline"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret SHAP (Partition SHAP approximation)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Token-level SHAP-style attribution in ferret, described as using a Partition SHAP approximation of Shapley values.",
        "main_functionalities": [
          "Compute SHAP-like token relevance scores for a prediction",
          "Support aggregation workflows for benchmarking and comparison"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret LIME token attribution",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token-level LIME explanations for transformer text classifiers via a local surrogate model.",
        "main_functionalities": [
          "Estimate token relevance using local perturbations and a surrogate model fit"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "transformers-interpret Integrated Gradients",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token attribution for Hugging Face transformers built around Integrated Gradients, returning signed per-token attributions for a chosen class/output.",
        "main_functionalities": [
          "Produce token-level attributions for transformer classifiers",
          "Provide notebook-friendly visualization outputs"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "transformers-interpret Layer Integrated Gradients",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Layer-level IG variant used in transformers-interpret for transformer models, described as a core attribution method alongside IG.",
        "main_functionalities": [
          "Compute attributions with respect to internal layers (implementation-specific)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Interpret-Text ClassicalTextExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Explainer for classical text pipelines using bag-of-words encoding and interpretable model parameters (e.g., logistic regression weights or tree feature importances) to surface word importances.",
        "main_functionalities": [
          "Handle text preprocessing/encoding in its default pipeline (bag-of-words)",
          "Train or wrap supported linear/tree models and expose word importances as explanations",
          "Provide local word importances and support label-wise/global inspection via dashboard workflows"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Interpret-Text UnifiedInformationExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Information-based measure used to explain how intermediate layers of deep NLP models encode information of input words; implemented for BERT in Interpret-Text.",
        "main_functionalities": [
          "Compute layer-wise, word-level quantitative explanations across BERT transformer layers (as implemented)",
          "Support inspection across transformer, pooler, and classification layers (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [
        "Implementation described as BERT-only in Interpret-Text."
      ],
      "research_applications": [
        {
          "used_in": "Towards a Deep and Unified Understanding of Deep Neural Models in NLP",
          "type": "paper",
          "year": 2019,
          "source": "PMLR (ICML 2019)",
          "url": "https://proceedings.mlr.press/v97/guan19a.html",
          "note": "Introduces the unified information-based measure referenced by the Interpret-Text explainer."
        }
      ]
    },
    {
      "name": "Interpret-Text IntrospectiveRationaleExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Generator–predictor selective rationalization framework that produces rationales (and anti-rationales) by incorporating predicted outcomes into the selection process.",
        "main_functionalities": [
          "Generate a subset of input tokens/spans intended to be sufficient for the classification outcome (rationales)",
          "Optionally identify complement/anti-rationales (tokens not useful for prediction), as described in the toolkit README"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1910.13294",
          "note": "Introduces the introspective rationalization approach referenced by Interpret-Text."
        }
      ]
    },
    {
      "name": "Interpret-Text Likelihood Explainer",
      "task_input": ["generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation-based local explanation for generative text models using conditional log-likelihood (log-probability) measurements to attribute importance to text segments.",
        "main_functionalities": [
          "Create perturbed versions of input text",
          "Measure changes in conditional log likelihood under perturbations to assign local importances",
          "Require access to model log probabilities (as described)"
        ]
      },
      "strengths": [
        "Model-agnostic given access to log probabilities (as described)."
      ],
      "limitations": [
        "Requires models/interfaces that provide log probabilities."
      ],
      "research_applications": []
    },
    {
      "name": "Interpret-Text Sentence Embedder Explainer",
      "task_input": ["generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation-based local explanation that does not require log probabilities; uses sentence embeddings to compare perturbed outputs to the original output and assign importance based on embedding proximity.",
        "main_functionalities": [
          "Generate perturbed versions of the input",
          "Compare outputs via embedding similarity/proximity to the original output",
          "Assign importance to segments based on impact on embedding proximity (as described)"
        ]
      },
      "strengths": [
        "Does not require log probabilities (as described), enabling use with API-only models that do not expose log-likelihood."
      ],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Interpret-Text Hierarchical Text Explainer",
      "task_input": ["generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Hierarchical perturbation explainer for generative models that analyzes text at multiple granularities (sentence level down to n-gram partitions) to identify influential components, optionally using either likelihood-based or embedding-based local explanation backends.",
        "main_functionalities": [
          "Apply perturbations across multiple text scales (sentence to n-gram partitions)",
          "Measure output changes to assign importance at varying levels of detail",
          "Support combination with likelihood-based or sentence-embedding-based local explanation backends (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "PnPXAI Attention Rollout",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Attention-based attribution method that aggregates attention information across layers/heads to produce token-level relevance estimates (as exposed by PnPXAI).",
        "main_functionalities": [
          "Compute attention-based relevance maps via attention aggregation/rollout (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "PnPXAI KernelShap",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Model-agnostic Kernel SHAP explainer exposed by PnPXAI.",
        "main_functionalities": [
          "Compute local Shapley-style feature attributions for a predictor"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models",
          "type": "paper",
          "year": 2025,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2505.10515",
          "note": "Describes PnPXAI and evaluates multiple explainers including KernelSHAP."
        }
      ]
    },
    {
      "name": "DALEX BreakDown",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Prediction-level variable attribution method that decomposes a single prediction into additive contributions along an ordered path (BreakDown).",
        "main_functionalities": [
          "Compute per-feature contributions for a single instance",
          "Support interaction-aware variants (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "DALEX Shap (random-path Shapley values)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Prediction-level Shapley-value attribution in DALEX computed by averaging attributions across multiple random feature orderings (as documented).",
        "main_functionalities": [
          "Compute Shapley-style attributions for a single instance using sampled paths"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "FAT Forensics CounterfactualExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Counterfactual explanation generator for black-box classifier predictions (as exposed in FAT Forensics).",
        "main_functionalities": [
          "Generate counterfactual instances intended to change the model prediction",
          "Provide utilities to textualize/describe counterfactuals (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and Transparency",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1909.05167",
          "note": "Describes the FAT Forensics toolbox and its scope including transparency components."
        }
      ]
    },
    {
      "name": "Activation Patching (TransformerLens)",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Causal intervention technique that patches internal activations from a ‘clean’ run into a ‘corrupted’ run and measures the effect on model outputs; TransformerLens provides utilities implementing activation patching across activation types.",
        "main_functionalities": [
          "Patch chosen activations between runs and measure downstream output changes",
          "Localize internal components important for a specific behavior/prediction"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Path Patching",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Refined patching variant where each patch is constrained to affect only a single target component, described as patching the ‘path’ between two components.",
        "main_functionalities": [
          "Test whether effects propagate directly between two components versus via mediation",
          "Conduct more targeted causal tracing than unconstrained activation patching"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "How to use and interpret activation patching",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2404.15255",
          "note": "Defines and discusses path patching as a constrained form of patching."
        }
      ]
    },
    {
      "name": "Attribution Patching",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Gradient-based approximation technique described as enabling many activation-patching measurements efficiently using a small number of forward/backward passes.",
        "main_functionalities": [
          "Approximate patching effects with gradient-based computations (as described)",
          "Scale patching-style analysis to many components/patches more efficiently"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Attribution Patching: Activation Patching At Industrial Scale",
          "type": "blog",
          "year": 2026,
          "source": "neelnanda.io",
          "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching",
          "note": "Introduces attribution patching as a gradient-based approximation to activation patching."
        }
      ]
    },
    {
      "name": "VISIT semantic information flow graph",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Interactive visualization method/tool that represents a forward pass as a flow graph (nodes as neurons/hidden states; edges as interactions) grounded in vocabulary-space projections to study information flow in GPT-style models.",
        "main_functionalities": [
          "Construct forward-pass flow graphs for inspection",
          "Support analysis of attention heads and ‘memory values’ via vocabulary projections (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers",
          "type": "paper",
          "year": 2023,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2305.13417",
          "note": "Introduces VISIT and its interactive flow-graph approach."
        }
      ]
    },
    {
      "name": "Vocabulary-space FFN update decomposition",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Mechanistic interpretability method that views token representations as distributions over vocabulary and interprets FFN outputs as additive updates in vocabulary space, decomposable into sub-updates linked to parameter vectors promoting interpretable concepts.",
        "main_functionalities": [
          "Project FFN-induced representation changes into vocabulary space",
          "Decompose updates into sub-components associated with FFN parameter vectors (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2203.14680",
          "note": "Introduces the vocabulary-space FFN update interpretation and decomposition."
        },
        {
          "used_in": "LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2204.12130",
          "note": "Uses this vocabulary-space method as the backbone for debugging and interventions."
        }
      ]
    },
    {
      "name": "BertViz multi-scale attention visualization",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Interactive visualization method/tool for inspecting multi-head self-attention at multiple granularities (head-, model-, and neuron-level views, as described).",
        "main_functionalities": [
          "Visualize attention patterns across layers and heads",
          "Provide multiple views to inspect attention structure and related signals"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Visualizing Attention in Transformer-Based Language Representation Models",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1904.02679",
          "note": "Describes BertViz and its multi-scale attention visualizations."
        }
      ]
    },
    {
      "name": "exBERT attention + contextual representation exploration",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Interactive analysis method/tool that explores attention weights and contextual representations and matches input contexts to similar contexts in an annotated dataset to aid interpretation.",
        "main_functionalities": [
          "Visualize attention and contextual embeddings for transformer inputs",
          "Retrieve/match similar contexts from a corpus and aggregate annotations (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models",
          "type": "paper",
          "year": 2019,
          "source": "ar5iv/arXiv HTML",
          "url": "https://ar5iv.labs.arxiv.org/html/1910.05276",
          "note": "Introduces exBERT and its context-matching interpretation mechanism."
        }
      ]
    },
    {
      "name": "MechaMap neuron category scanning",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "global",
      "description": {
        "overview": "Single-pass scanning workflow intended to surface neurons that respond strongly to predefined semantic categories by running a ‘master text’ and measuring activations at MLP outputs (as described in the project README).",
        "main_functionalities": [
          "Run a configured ‘master text’ and compute neuron activations by category",
          "Export ranked neurons / top-activating tokens for downstream inspection (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    }
  ]
}
