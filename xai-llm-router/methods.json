{
  "toolkits": [
     {
      "name": "Integrated Gradients (IG)",
      "plugin_id": "captum_ig_classifier",
      "task_input": ["classification"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility": "non experts", 
      "description": {
        "overview": "Integrated Gradients (IG) attributes a model’s prediction to input tokens by integrating gradients along a path from a baseline to the actual input. It produces per-token attributions for the target output.",
        "main_functionalities": [
          "Compute token-level importance by integrating gradients from a baseline",
          "Support configurable number of integration steps",
          "Applicable to any differentiable model (white-box access required)"
        ]
      },
      "strengths": [
        "Satisfies axioms of sensitivity and implementation invariance",
        "More stable and less noisy than raw gradients",
        "Provides intuitive baseline comparison ('change from reference to input')"
      ],
      "limitations": [
        "Requires choosing a baseline, which affects results",
        "Computationally expensive due to multiple forward/backward passes",
        "Can provide noisy explanations"
      ],
      "research_applications": [
        {
          "used_in": "Axiomatic Attribution for Deep Networks",
          "type": "paper",
          "year": 2017,
          "source": "ICML",
          "url": "https://dl.acm.org/doi/10.5555/3305890.3306024",
          "note": "Introduces the Integrated Gradients method."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/integrated_gradients.html",
          "note": "Details the Integrated Gradients implementation in Captum."
        }, 
        {
          "used_in": "ferret: a Framework for Benchmarking Explainers on Transformers",
          "type": "paper",
          "year": 2023,
          "source": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
          "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
          "note": "Introduces the ferret toolkit which leverages feature attribution approaches like Integrated Gradients to support the interpretation of individual model predictions."
        }, 
        {
          "used_in": "The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models",
          "type": "paper",
          "year": 2020,
          "source": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)",
          "url": "https://doi.org/10.18653/v1/2020.emnlp-demos.15",
          "note": "Introduces the LIT toolkit which implements Integrated Gradients."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "Layer Integrated Gradients",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility": "mid experts",
      "description": {
        "overview": "Layer Integrated Gradients is a variation of Integrated Gradients that computes importance scores for the inputs or outputs of a specific layer, rather than for the original input features. In contrast, Integrated Gradients assigns an importance score to each input feature by approximating the integral of the gradients of the model’s output with respect to those inputs along a path from chosen reference inputs to the actual inputs.",
          "main_functionalities": [
          "Compute attributions for internal layer inputs or outputs",
          "Enable analysis of intermediate representations"
        ]
      },
      "strengths": [  "Like integrated gradients, provides theoretically grounded attributions satisfying axioms",
    "Enables interpretability of intermediate representations rather than only input features",
    "More stable than raw gradient-based methods due to path integration",
    "Can be combined with token-level or neuron-level analysis for fine-grained inspection"],
      "limitations": [ "Like integrated gradients, sensitive to the choice of baseline/reference input" , 
      "Like integrated gradients, it is computationally more expensive than raw gradient-based tools"
     ],
      "research_applications": [
         {
          "used_in": "Axiomatic Attribution for Deep Networks",
          "type": "paper",
          "year": 2017,
          "source": "ICML",
          "url": "https://dl.acm.org/doi/10.5555/3305890.3306024",
          "note": "Introduces the Integrated Gradients tool on which Layer Integrated Gradients builds."
        }, 
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/layer_integrated_gradients.html",
          "note": "Documents the layer-wise IG method in Captum."
        }, 
        {
          "used_in": "Captum Tutorial",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/tutorials/Llama2_LLM_Attribution",
          "note": "Tutorial where layer integrated gradients is leveraged to explain Llama2."
        }
      ],
      "typical_cost": "medium", 
      "implementation": "https://github.com/meta-pytorch/captum"
    },
    {
      "name": "Saliency",
      "plugin_id": "captum_saliency_classifier",
      "task_input": ["classification"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Saliency computes the gradient of the output with respect to each input embedding, producing a token-level attribution scores.",
        "main_functionalities": [
          "Compute token importance as the magnitude of gradients of the output with respect to the input"
        ]
      },
      "strengths": [
        "Very fast",
        "Easy to interpret as 'which tokens influence the output most'",
        "Admits an intuitive interpretation as the first-order Taylor approximation of the model around the input. The gradients represent the coefficients of this local linear approximation, and their magnitude is used as a measure of how sensitive the prediction is to each input token.", 
        "Useful for quick debugging and visualization"
      ],
      "limitations": [
        "Often noisy or unstable, prone to highlight spurious importances",
        "Suffers from gradient saturation, which means that gradients approach zero and can be misleading in a neighbourhood of the inputs",
        "Does not satisfy desirable axioms like sensitivity and implementation invariance"

      ],
      "research_applications": [
        {
          "used_in": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
          "type": "paper",
          "year": 2014,
          "source": "ICLR Workshop",
          "url": "https://arxiv.org/pdf/1312.6034",
          "note": "Introduces the basic saliency/gradient method."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/saliency.html",
          "note": "Describes Saliency in Captum."
        }, 
        {
          "used_in": "Using Captum to Explain Generative Language Models",
          "type": "paper",
          "year": 2023,
          "source": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
          "url": "https://aclanthology.org/2023.nlposs-1.19.pdf",
          "note": "Introduces attribution tools like Saliency in the popular Captum toolkit."
        },
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkit which also implements Saliency."
        }
      ],
      "implementation": "https://github.com/meta-pytorch/captum", 
      "typical_cost": "low"
    },
    {
      "name": "Saliency",
      "task_input": ["generation"],
       "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Saliency computes the gradient of the output with respect to each input embedding, producing a token-level attribution scores.",
        "main_functionalities": [
          "Compute token importance as the magnitude of gradients of the output with respect to the input"
        ]
      },
      "strengths": [
        "Very fast",
        "Easy to interpret as 'which tokens influence the output most'",
        "Admits an intuitive interpretation in terms of the first-order Taylor approximation of the model around the input. The gradients represent the coefficients of this local linear approximation, and their magnitude is used as a measure of how sensitive the prediction is to each input token.", 
        "Useful for quick debugging and visualization"
      ],
      "limitations": [
        "Often noisy or unstable, prone to highlight spurious importances",
        "Suffers from gradient saturation, which means that gradients approach zero and can be misleading in a neighbourhood of the inputs",
        "Does not satisfy desirable axioms like sensitivity and implementation invariance"

      ],
      "research_applications": [
        {
          "used_in": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
          "type": "paper",
          "year": 2014,
          "source": "ICLR Workshop",
          "url": "https://arxiv.org/pdf/1312.6034",
          "note": "Introduces the basic saliency/gradient method."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/saliency.html",
          "note": "Describes Saliency in Captum."
        }, 
        {
          "used_in": "Using Captum to Explain Generative Language Models",
          "type": "paper",
          "year": 2023,
          "source": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
          "url": "https://aclanthology.org/2023.nlposs-1.19.pdf",
          "note": "Introduces attribution tools like Saliency in the popular Captum toolkit."
        },
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Saliency."
        }
      ],
      "implementation": "https://github.com/meta-pytorch/captum", 
      "typical_cost": "low"
    },
    {
      "name": "Input\u00d7Gradient",
      "task_input": ["classification"],
      "plugin_id": "captum_inputxgradient_classifier", 
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Input\u00d7Gradient multiplies each input embedding by its gradient, yielding token-level attribution scores.",
        "main_functionalities": [
          "Compute contributions by combining input values with gradients"
        ]
      },
      "strengths": ["Very fast (single-pass computation)", "Capture both current token values and sensitivity of the output to those tokens",
       "Very intuitive to interpret: it captures contribution rather than only sensitivity."],
      "limitations": [   "Often noisy or unstable, prone to highlight spurious importances",
        "Suffers from gradient saturation, which means that gradients approach zero and can be misleading in a neighbourhood of the inputs",
        "Does not satisfy desirable axioms like sensitivity and implementation invariance"],
      "research_applications": [
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/input_x_gradient.html",
          "note": "Details the Input\u00d7Gradient method in Captum."
        }, 
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Input\u00d7Gradient."
        }
      ],
      "typical_cost": "low", 
      "implementation": "https://github.com/meta-pytorch/captum"
    },
    {
      "name": "Input\u00d7Gradient", 
      "task_input": ["generation"],
       "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Input\u00d7Gradient multiplies each input embedding by its gradient, yielding token-level attribution scores.",
        "main_functionalities": [
          "Compute contributions by combining input values with gradients"
        ]
      },
      "strengths": ["Very fast (single-pass computation)", "Capture both current token values and sensitivity of the output to those tokens",
       "Very intuitive to interpret: it captures contribution rather than only sensitivity."],
      "limitations": [   "Often noisy or unstable, prone to highlight spurious importances",
        "Suffers from gradient saturation, which means that gradients approach zero and can be misleading in a neighbourhood of the inputs",
        "Does not satisfy desirable axioms like sensitivity and implementation invariance"],
      "research_applications": [
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/input_x_gradient.html",
          "note": "Details the Input\u00d7Gradient method in Captum."
        }, 
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Input\u00d7Gradient."
        }
      ],
      "typical_cost": "low", 
      "implementation": "https://github.com/meta-pytorch/captum"
    },
    {
      "name": "DeepLIFT",
      "plugin_id": "captum_deeplift_classifier",
      "task_input": ["classification"],
      "access_arch": {"access": ["white_box"], "arch": ["encdec", "encoder", "decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "DeepLIFT attributes the change in a model’s output to differences in input features relative to a chosen baseline. Instead of propagating gradients, it backpropagates activation differences through the network using specific propagation rules to assign contribution scores to each input.",
        "main_functionalities": [
          "Compute token attributions relative to a reference baseline"
        ]
      },
      "strengths": [
        "Often faster than Integrated Gradients",
        "Handles cases of zero gradients (saturation) more robustly than raw-gradient tools", 
        "It satisfies additivity, a desirable property similar to completeness for Integrated Gradients"
      ],
      "limitations": [
        "Baseline choice can affect attributions (like for Integrated Gradients)"
      ],
      "research_applications": [
        {
          "used_in": "Learning Important Features Through Propagating Activation Differences",
          "type": "paper",
          "year": 2017,
          "source": "ICML",
          "url": "https://proceedings.mlr.press/v70/shrikumar17a.html",
          "note": "Introduces DeepLIFT method."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/deeplift.html",
          "note": "Describes DeepLIFT in Captum."
        }
      ],
      "typical_cost": "medium", 
      "implementation": "https://github.com/kundajelab/deeplift"
    },
    {
      "name": "GradientSHAP",
      "plugin_id": "captum_gradientshap_classifier", 
      "task_input": ["classification"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility": "non experts",
      "description": {
        "overview": "GradientSHAP combines integrated gradients with randomized baselines. It approximates Shapley values by averaging integrated gradients from multiple noisy baseline inputs.",
        "main_functionalities": [
          "Sample multiple baselines and integrate gradients",
          "Approximate Shapley-value attributions via gradient estimates"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP framework combining kernel and gradient methods."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/gradient_shap.html",
          "note": "Describes GradientSHAP in Captum."
        }, 
         {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Gradient SHAP."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "GradientSHAP",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility": "non experts",
      "description": {
        "overview": "GradientSHAP combines integrated gradients with randomized baselines. It approximates Shapley values by averaging integrated gradients from multiple noisy baseline inputs.",
        "main_functionalities": [
          "Sample multiple baselines and integrate gradients",
          "Approximate Shapley-value attributions via gradient estimates"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP framework combining kernel and gradient methods."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/gradient_shap.html",
          "note": "Describes GradientSHAP in Captum."
        }, 
         {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Gradient SHAP."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Feature Ablation (Captum)",
      "task_input": ["classification"],
      "plugin_id": "captum_featureablation_classifier", 
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Feature Ablation is a perturbation method that measures output change when input features (tokens) are removed or replaced with a baseline. It estimates importance by deletion impact.",
        "main_functionalities": [
          "Remove or mask input tokens and observe output differences",
          "Compute importance scores as change in prediction"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Global Explanation of Neural Networks via Feature Ablation",
          "type": "paper",
          "year": 2019,
          "source": "ArXiv",
          "url": "https://arxiv.org/abs/1806.03824",
          "note": "Discusses feature ablation methods for model interpretability."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Feature Ablation (Captum)",
      "task_input": ["generation"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Feature Ablation is a perturbation method that measures output change when input features (tokens) are removed or replaced with a baseline. It estimates importance by deletion impact.",
        "main_functionalities": [
          "Remove or mask input tokens and observe output differences",
          "Compute importance scores as change in prediction"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Global Explanation of Neural Networks via Feature Ablation",
          "type": "paper",
          "year": 2019,
          "source": "ArXiv",
          "url": "https://arxiv.org/abs/1806.03824",
          "note": "Discusses feature ablation methods for model interpretability."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Occlusion",
      "plugin_id": "captum_occlusion_classifier", 
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Occlusion masks out contiguous input segments (e.g., spans of tokens) and measures the effect on model output. The difference indicates the importance of the occluded region.",
        "main_functionalities": [
          "Systematically mask segments of input text",
          "Measure output change to infer importance of segments"
        ]
      },
      "strengths": [
        "Intuitive and human-friendly",
         "Model-agnostic (no gradients needed)",
        "Highlights contiguous spans that influence output"
      ],
      "limitations": [
        "Requires many forward passes (computationally expensive)",
        "Masked inputs (e.g., with UNK) can be unnatural",
        "Local method only (does not summarize global behavior)"
      ],
      "research_applications": [
        {
          "used_in": "Visualizing and Understanding Convolutional Networks",
          "type": "paper",
          "year": 2014,
          "source": "ECCV",
          "url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53",
          "note": "Popularizes occlusion analysis for CNNs."
        }, 
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Kernel SHAP."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Occlusion",
      "task_input": ["generation"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Occlusion masks out contiguous input segments (e.g., spans of tokens) and measures the effect on model output. The difference indicates the importance of the occluded region.",
        "main_functionalities": [
          "Systematically mask segments of input text",
          "Measure output change to infer importance of segments"
        ]
      },
      "strengths": [
        "Intuitive and human-friendly",
         "Model-agnostic (no gradients needed)",
        "Highlights contiguous spans that influence output"
      ],
      "limitations": [
        "Requires many forward passes (computationally expensive)",
        "Masked inputs (e.g., with UNK) can be unnatural",
        "Local method only (does not summarize global behavior)"
      ],
      "research_applications": [
        {
          "used_in": "Visualizing and Understanding Convolutional Networks",
          "type": "paper",
          "year": 2014,
          "source": "ECCV",
          "url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53",
          "note": "Popularizes occlusion analysis for CNNs."
        }, 
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Kernel SHAP."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "SHAP",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility": "non experts",
      "description": {
        "overview": "SHAP is a model-agnostic method to estimate feature attributions by Shapley values.",
        "main_functionalities": [
          "Compute local Shapley-based attributions for a predictor"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP (Lundberg & Lee, 2017)."
        }, 
        {
          "used_in": "ferret: a Framework for Benchmarking Explainers on Transformers",
          "type": "paper",
          "year": 2023,
          "source": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
          "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
          "note": "Introduces the ferret toolkit which leverages feature attribution approaches like LIME to support the interpretation of individual model predictions."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "TokenSHAP",
      "task_input": ["generation"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility": "non experts",
      "description": {
        "overview": "TokenSHAP is an implementation of SHAP built for text generation via language models.",
        "main_functionalities": [
          "Compute local Shapley-based attributions for a predictor"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP (Lundberg & Lee, 2017)."
        }, 
        {
          "used_in": "TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation",
          "type": "paper",
          "year": 2022, 
          "source": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
          "url": "https://arxiv.org/pdf/2407.10114",
          "note": "Introduces TokenSHAP, a Monte Carlo implementation of SHAP tailored for large language models."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Kernel SHAP",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "all",
      "accessibility":"non experts",
      "description": {
        "overview": "Kernel SHAP is a model-agnostic method to estimate feature attributions by approximating Shapley values with a weighted linear regression on perturbations.",
        "main_functionalities": [
          "Compute local Shapley-based attributions for a predictor",
          "Aggregate local explanations for a global summary (if needed)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP (Lundberg & Lee, 2017)."
        }, 
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Kernel SHAP."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Sobol",
      "task_input": ["generation","classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Based on the theoretical framework of sensitivity analysis via Sobol indices, Sobol indices measure both individual (first-order) and interaction (higher-order) effects of features on model outputs.",
        "main_functionalities": [
          "Compute local attributions"
        ]
      },
      "strengths": ["Theoretically grounded in sensitivity analysis", 
    "Accounts for interaction effects beyond univariate attributions"],
      "limitations": ["Not tailored to natural language"],
      "research_applications": [
        {
          "used_in":  "Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis", 
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://arxiv.org/pdf/2111.04138",
          "note": "Introduces the Sobol tool."
        }, 
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Kernel SHAP."
        }
      ],
      "typical_cost": "missing"
    },
    {
      "name": "SmoothGrad",
      "task_input": ["generation","classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Instead of using raw gradients to compute attributions, which can be unstable, SmoothGrad approximates a smoothing of the raw gradients based on a Gaussian kernel.",
        "main_functionalities": [
          "Compute local attributions"
        ]
      },
      "strengths": ["More stable than attribution tools based on raw gradients", 
      "Reduces visual artifacts and sharp discontinuities in attributions, leading to explanations that are easier to interpret", 
       "It can be viewed as a general procedure which computes an attribution map by averaging over multiple attribution maps of an arbitrary gradient-based attribution method with multiple noised input"],
      "limitations": ["More computationally demanding than attribution tools based on raw gradients, requiring multiple forward passes", 
    "Obtaining proper results requires carefully selecting the variance of the Gaussian noise", 
    "Not tailored to natural language"],
      "research_applications": [
        {
          "used_in": "SmoothGrad: removing noise by adding noise",
          "type": "paper",
          "year": 2017,
          "source": "",
          "url": "https://arxiv.org/pdf/1706.03825",
          "note": "Introduces the SmoothGrad tool."
        }, 
        {
          "used_in": "Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution",
          "type": "paper",
          "year": 2021,
          "source": "",
          "url": "https://arxiv.org/pdf/2004.10484",
          "note": "Introduces the TaylorGrad tool, a particular instance of SmoothGrad that bridges the gap between SmoothGrad and Integrated Gradients."
        }, 
        {
          "used_in": "Interpreto: An Explainability Library for Transformers",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv",
          "url": "https://arxiv.org/html/2512.09730v1", 
          "note": "Introduces the Interpreto toolkits which also implements Kernel SHAP."
        }
      ],
      "typical_cost": "missing"
    },
    {
      "name": "LIME",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts", 
      "description": {
        "overview": "LIME (Local Interpretable Model-agnostic Explanations) fits a simple surrogate model around a specific instance by sampling perturbations. The surrogate’s weights on features explain the local prediction.",
        "main_functionalities": [
          "Generate local explanations by fitting an interpretable model (e.g., linear) around an instance",
          "Learn feature weights from perturbed samples"
        ]
      },
      "strengths": [
        "Provides interpretable local explanations via simple models",
        "Model-agnostic and widely applicable"
      ],
      "limitations": [
        "Requires many perturbed samples (computationally heavy)",
        "Explanations sensitive to sampling and kernel choices"
      ],
      "research_applications": [
        {
          "used_in": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
          "type": "paper",
          "year": 2016,
          "source": "KDD",
          "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
          "note": "Introduces the LIME method."
        }, 
        {
          "used_in": "ferret: a Framework for Benchmarking Explainers on Transformers",
          "type": "paper",
          "year": 2023,
          "source": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
          "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
          "note": "Introduces the ferret toolkit which leverages feature attribution approaches like LIME to support the interpretation of individual model predictions."
        }, 
        {
          "used_in": "InterpretML: {A} Unified Framework for Machine Learning Interpretability",
          "type": "paper",
          "year": 2019,
          "source": "ArXiv",
          "url": "http://arxiv.org/abs/1909.09223",
          "note": "Introduces the  InterpretML toolkit which implements the LIME method."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Partial Dependence Plot",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "global",
      "description": {
        "overview": "Partial Dependence Plots (PDP) estimate the marginal effect of one or more features on the predicted outcome by averaging model outputs over a dataset.",
        "main_functionalities": [
          "Compute average model predictions for varying feature values",
          "Visualize global feature–prediction relationships"
        ]
      },
      "strengths": [
        "Provides a global view of feature effects",
        "Easy to interpret trends of feature influence"
      ],
      "limitations": [
        "Assumes feature independence; can mislead if features correlate",
        "Only captures marginal (average) effects, not interactions"
      ],
      "research_applications": [
        {
          "used_in": "Greedy Function Approximation: A Gradient Boosting Machine (Friedman)",
          "type": "paper",
          "year": 2001,
          "source": "Annals of Statistics",
          "url": "https://projecteuclid.org/euclid.aos/1013203451",
          "note": "Introduces partial dependence plots."
        }, 
        {
          "used_in": "InterpretML: {A} Unified Framework for Machine Learning Interpretability",
          "type": "paper",
          "year": 2019,
          "source": "ArXiv",
          "url": "http://arxiv.org/abs/1909.09223",
          "note": "Introduces the  InterpretML toolkit which implements the LIME method."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "Morris Sensitivity Analysis",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "global",
      "accessibility":"non experts",
      "description": {
        "overview": "Morris Sensitivity analysis is a global sensitivity method that samples feature space along trajectories and computes elementary effects. It estimates each feature’s influence on the output variance.",
        "main_functionalities": [
          "Compute Morris 'elementary effects' by varying one feature at a time",
          "Estimate global sensitivity indices for features"
        ]
      },
      "strengths": [
        "Model-agnostic global feature importance",
        "Requires fewer samples than full factorial designs"
      ],
      "limitations": [
        "Computationally expensive in high dimensions",
        "Results depend on trajectory design and sampling"
      ],
      "research_applications": [
        {
          "used_in": "Factorial Sampling Plans for Preliminary Computational Experiments (Morris)",
          "type": "paper",
          "year": 1991,
          "source": "Technometrics",
          "url": "https://doi.org/10.1080/00401706.1991.10484804",
          "note": "Introduces Morris sensitivity method."
        }, 
        {
          "used_in": "InterpretML: {A} Unified Framework for Machine Learning Interpretability",
          "type": "paper",
          "year": 2019,
          "source": "ArXiv",
          "url": "http://arxiv.org/abs/1909.09223",
          "note": "Introduces the  InterpretML toolkit which implements the Morris Sensitivity analysis method."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Anchors",
      "plugin_id": "alibi_anchors_text",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["transformer_general"]},
      "target_scope": "local",
      "description": {
        "overview": "Anchors identifies a set of words or feature conditions (an 'anchor') that fix a model’s prediction locally. It generates IF-THEN rules guaranteeing the prediction with high precision.",
        "main_functionalities": [
          "Find words/phrases whose presence anchors the prediction",
          "Provide precision (reliability) and coverage metrics for anchors",
          "Show example inputs where the anchor applies or fails"
        ]
      },
      "strengths": [
        "Produces intuitive IF-THEN rule explanations",
        "Model-agnostic: works with any classifier",
        "Quantifies explanation reliability and scope"
      ],
      "limitations": [
        "Local explanation only (no global insight)",
        "Computationally expensive for long texts (sampling large neighborhoods)",
        "Quality depends on perturbation strategy (masking may be unnatural)"
      ],
      "research_applications": [
        {
          "used_in": "Anchors: High-Precision Model-Agnostic Explanations",
          "type": "paper",
          "year": 2018,
          "source": "AAAI",
          "url": "https://homes.cs.washington.edu/~marcotcr/aaai18.pdf",
          "note": "Original paper introducing Anchors."
        },
        {
          "used_in": "A Sea of Words: Analysis of Anchors for Text",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2205.13789",
          "note": "Analyzes theoretical properties of Anchors for text classification."
        },
        {
          "used_in": "Alibi Explain: Model Explanation Library",
          "type": "paper",
          "year": 2021,
          "source": "JMLR",
          "url": "https://jmlr.org/papers/v22/21-0017.html",
          "note": "Describes Alibi library, which implements Anchors."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Direct Logit Attribution",
      "plugin_id": "direct_logit_attribution",
      "task_input": ["generation", "seq2seq", "general_NLP", "NA"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general", "NA"]},
      "target_scope": "local",
      "description": {
        "overview": "Direct Logit Attribution (DLA) projects each model component’s contribution onto the target token’s logit in the unembedding space. Positive contributions push the logit up (toward the token), negatives push it down.",
        "main_functionalities": [
          "Project each layer, attention head, or MLP output onto the target token’s output weight",
          "Compute signed contributions of components to a chosen token’s logit",
          "Rank components by their absolute contribution magnitude"
        ]
      },
      "strengths": [
        "Fast and simple: one forward pass plus linear projections",
        "Intuitive signed contributions (direct evidence vs. counter-evidence)",
        "Helps locate where the prediction is being written in the model"
      ],
      "limitations": [
        "Not causal: large contribution might be overridden later",
        "Ignores interactions (softmax competition, nonlinear effects)",
        "Depends on normalization placement (affects interpretability)",
        "Can mislead if components clean up earlier signals"
      ],
      "research_applications": [
        {
          "used_in": "Attribution Patching: Activation Patching at Industrial Scale",
          "type": "blog",
          "year": 2023,
          "source": "Neel Nanda blog",
          "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching",
          "note": "Discusses Direct Logit Attribution as a diagnostic method."
        },
        {
          "used_in": "TransformerLens ActivationCache.decompose_resid",
          "type": "docs",
          "year": 2023,
          "source": "TransformerLens",
          "url": "https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.ActivationCache.html",
          "note": "Tool support for decomposing residual streams, enabling DLA."
        },
        {
          "used_in": "A Mathematical Framework for Transformer Circuits",
          "type": "article",
          "year": 2021,
          "source": "Transformer Circuits blog",
          "url": "https://transformer-circuits.pub/2021/framework/index.html",
          "note": "Presents the residual stream view motivating attribution approaches."
        },
        {
          "used_in": "Adversarial Example for Direct Logit Attribution",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2310.07325",
          "note": "Demonstrates cases where DLA can be misleading (residual erasure)."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "Logit Lens",
      "plugin_id": "logit_lens",
      "task_input": ["generation", "seq2seq", "general_NLP", "NA"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general", "NA"]},
      "target_scope": "local",
      "description": {
        "overview": "Logit Lens projects each intermediate hidden state through the model’s output head to view predicted token probabilities at each layer. It tracks how the model’s top prediction evolves through the layers【64†L53-L61】.",
        "main_functionalities": [
          "Compute top-k token predictions for each layer at a chosen position",
          "Track the target token’s logit or probability across layers",
          "Compare layer-wise predictions under optional normalization"
        ]
      },
      "strengths": [
        "Very fast: one forward pass provides layer-wise outputs",
        "Shows when and how representations become linearly decodable",
        "Natural for decoder-only LMs (no additional sampling)"
      ],
      "limitations": [
        "Not causal: reveals decode-ability, not true necessity",
        "Intermediate states require proper normalization for comparison",
        "Top-k outputs can be misleading when probabilities are diffuse"
      ],
      "research_applications": [
        {
          "used_in": "Interpreting GPT: The Logit Lens",
          "type": "blog",
          "year": 2020,
          "source": "LessWrong",
          "url": "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens",
          "note": "Introduces the logit lens idea for GPT."
        },
        {
          "used_in": "Information Flow of Transformers (EMNLP 2023 Findings)",
          "type": "paper",
          "year": 2023,
          "source": "ACL Anthology",
          "url": "https://aclanthology.org/2023.findings-emnlp.939.pdf",
          "note": "Applies logit lens to study information flow."
        },
        {
          "used_in": "What Language Do Non-English LLMs Think In? (ACL 2025 Findings)",
          "type": "paper",
          "year": 2025,
          "source": "ACL Anthology",
          "url": "https://aclanthology.org/2025.findings-acl.1350.pdf",
          "note": "Uses logit lens to detect latent language signals."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "Layer-wise Relevance Propagation (LRP)",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "LRP redistributes the prediction score backward through the network to the inputs, assigning relevance scores via fixed propagation rules. It can compute attributions for the input token as well as at any layer",
        "main_functionalities": [
          "Backpropagate a target score to input features through relevance rules",
          "Ensure relevance conservation (inputs sum to output)",
          "Handle transformer components like positional embeddings specially"
        ]
      },
      "strengths": [
        "Theoretically grounded propagation rules enforce conservation",
        "Inuitive and efficient"
      ],
      "limitations": [
        "Not straightforward to implement for different models (requires layer-specific rules)"
      ],
      "research_applications": [
          {
          "used_in": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
          "type": "paper",
          "year": 2015,
          "source": "PloS one",
          "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140",
          "note": "Introduce the layer-wise relevance propagation method."
        },
          {
          "used_in": "AttnLRP: attention-aware layer-wise relevance propagation for transformers",
          "type": "paper",
          "year": 2024,
          "source": "Proceedings of the 41st International Conference on Machine Learning (ICML)",
          "url": "",
          "note": "Extends the Layer-wise Relevance Propagation attribution method to handle attention layers."
        },
        {
          "used_in": "Revisiting LRP: Positional Attribution for Transformers",
          "type": "paper",
          "year": 2025,
          "source": "arXiv",


          "url": "https://arxiv.org/abs/2506.02138",
          "note": "Adaptation of LRP to Transformer models, accounting for positional encodings."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "Attention Rollout",
      "plugin_id": "attention_rollout",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "accessibility":"non experts",
      "description": {
        "overview": "Late layers often show homogenous and hence uninformative attention patterns because they become more contextualized. Attention rollout addresses this by tracking down attention weights all the way back to the input layer. In particular, Attention Rollout aggregates multi-head self-attention across layers by multiplying attention matrices and outputs a single attention score for any pair of tokens.",   
        "main_functionalities": [
          "Aggregate attention weights across heads and layers",
          "Compute token-to-token relevance scores via attention product"
        ]
      },
      "strengths": [
        "Efficient: uses existing attention weights (no backprop required)",
        "Intuitive:  at the implementation level, the method boilds down to multiplying attention matrices across layers.",
        "Provides intuitive visualization of attention dependencies", 
        "Considers residual connections"
      ],
      "limitations": [
        "Inherits the lack of faithfulness of attention-based explanations (attention may not reflect true importance)",
        "Not causal",
        "Ignores MLPs"
      ],
      "research_applications": [
        {
          "used_in": "Quantifying Attention Flow in Transformers",
          "type": "paper",
          "year": 2020,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2005.00928",
          "note": "Introduces the Attention Rollout method."
        }, 
        {
          "used_in": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models",
          "type": "paper",
          "year": 2025,
          "source": "arXiv",
          "url": "missing",
          "note": "Introduces the PnPXAI which implements the Attention Rollout method."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "PnPXAI Kernel SHAP",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "all",
      "description": {
        "overview": "Kernel SHAP (in PnPXAI) is a model-agnostic Shapley-value estimator, using a weighted linear regression on perturbations. It provides local explanations similarly to general Kernel SHAP.",
        "main_functionalities": [
          "Compute local Shapley explanations for model predictions"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "PnPXAI: A Universal XAI Framework",
          "type": "paper",
          "year": 2025,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2505.10515",
          "note": "Describes PnPXAI framework, including Kernel SHAP."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "FAT Forensics CounterfactualExplainer",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "CounterfactualExplainer generates counterfactual instances that change a black-box model’s prediction. These are minimal edits to the input to flip the predicted class.",
        "main_functionalities": [
          "Generate counterfactual examples for a given prediction",
          "Report the differences needed to alter the model decision"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "FAT Forensics: Fairness, Accountability and Transparency Toolbox",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1909.05167",
          "note": "Describes the FAT Forensics toolbox including counterfactual modules."
        }
      ],
      "typical_cost": "high"
    },
     {
      "name": "LIT Counterfactual Generator",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "The tool supports counterfactual generation techniques to provide behavioral analysis through systematic input manipulation. The idea is to create modified versions of the original input by substituting, removing, or adding tokens and then observing how the model's predictions change in response to these perturbations, making it particularly suitable for practitioners who want to understand model decision boundaries without requiring access to internal model components.  It is particularly suitable for practitioners with limited or no prior experience in explainable AI.",
        "main_functionalities": [
          "Generate counterfactual examples for a given prediction",
          "Report the differences needed to alter the model decision"
        ]
      },
      "strengths": ["Explanations are intuitive",
    "The method is model agnostic"],
      "limitations": ["Scalability issues may arise"],
      "research_applications": [
        {
          "used_in": "The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models",
          "type": "paper",
          "year": 2020,
          "source": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)",
          "url": "https://doi.org/10.18653/v1/2020.emnlp-demos.15",
          "note": "Introduces the LIT toolkit which implements Integrated Gradients."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Activation Patching",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "Activation Patching compares two model runs by copying ('patching') certain internal activations from one run to another. The change in output reveals the causal effect of those activations.",
        "main_functionalities": [
          "Replace specified activations from a 'clean' run into a 'corrupted' run",
          "Measure output differences to identify critical components"
        ]
      },
      "strengths": [
        "Causal: directly tests necessity of components",
        "Allows pinpointing precise layers/neurons affecting a behavior"
      ],
      "limitations": [
        "Computationally expensive (requires many runs)",
        "Results depend on choice of baseline and interventions"
      ],
      "research_applications": [
        {
          "used_in": "How to Use and Interpret Activation Patching",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2404.15255",
          "note": "Provides methodology and best practices for activation patching."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Path Patching",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "Path Patching is a variant where patches are constrained to a single component's output, testing direct vs. indirect influence. It assesses whether two components have a direct causal connection.",
        "main_functionalities": [
          "Patch activations while limiting effect to one target component",
          "Distinguish direct from mediated causal pathways"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "How to Use and Interpret Activation Patching",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2404.15255",
          "note": "Introduces path-constrained activation patching."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Attribution Patching",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "Attribution Patching approximates activation patching using gradient-based calculations. It estimates many patching effects efficiently by propagating attribution scores.",
        "main_functionalities": [
          "Use gradients to approximate the effect of multiple patches",
          "Enable large-scale causal analysis with fewer runs"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Attribution Patching: Activation Patching at Industrial Scale",
          "type": "blog",
          "year": 2026,
          "source": "Neel Nanda blog",
          "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching",
          "note": "Introduces attribution patching."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "VISIT semantic information flow graph",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "VISIT visualizes a forward pass as a semantic flow graph. Nodes represent neurons or activations and edges represent interactions, projected into vocabulary space to trace information flow.",
        "main_functionalities": [
          "Construct interactive flow graph of activations and attentions",
          "Enable analysis of attention heads and memory values via vocabulary projections"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "VISIT: Visualizing Semantic Information Flow of Transformers",
          "type": "paper",
          "year": 2023,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2305.13417",
          "note": "Introduces the VISIT flow-graph visualization tool."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "MLP update decomposition",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "This method decomposes a Transformer MLP (feed-forward) layer as updates that pertain to single paramter vectors. Each vector is then interpreted as promoting specific  concepts.",
        "main_functionalities": [
          "Project MLP output changes into vocabulary logit space",
          "Decompose the MLP update into components which can be interpreted", 
          "Allows steering generation towards a direction of interest"
        ]
      }, 
      "strengths": ["Valuable to gain fine-grained insight on the mechanisms implemented in the MLP, which is a core yet poorly-understood component of the transformer.", 
    "Vectors in the decomposition of the MLP update are often interpretable."],
      "limitations": ["Possible non-linear interactions between MLP weight vectors that may generate new meanings are ignored."],
      "research_applications": [
        {
          "used_in": "Transformer FFN layers build predictions by promoting vocabulary-space concepts",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2203.14680",
          "note": "Introduces vocabulary-space FFN decomposition."
        },
        {
          "used_in": "LM-Debugger: Inspection and Intervention tool",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2204.12130",
          "note": "Uses vocabulary-space analysis for Transformer debugging."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "BertViz",
      "plugin_id": "bertviz_attention",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "BertViz is a tool for visualizing self-attention in transformers at multiple scales. It provides views of individual heads, all heads, or neuron-level interactions.",
        "main_functionalities": [
          "Visualize attention patterns between tokens across layers and heads",
          "Provide multiple interactive views (head-level, model-level, neuron-level)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Multiscale Visualization of Attention in the Transformer Model",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1904.02679",
          "note": "Introduces BertViz tool and its views."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "exBERT",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "exBERT is a tool which provides a concise overview of attention (attention view) and the internal representations (corpus view).",
        "main_functionalities": [
          "Visualize attention weights and token embeddings for a given input",
          "Computes summary statistics summarizing internal representations for a given token", 
          "The tool can be easily accessed online at https://exbert.vizhub.ai/client/exBERT.html"
        ]
      },
      "strengths": ["Easy to access", 
    "Useful to quickly test hypotheses on the internal representation of a specific token in a specific context"],
      "limitations": ["Attention is known not to provide faithful explananations", 
    "exBERT only provides statistics for a single token, considering a small number of neighbours, yielding no insights on global behaviors"],
      "research_applications": [
        {
          "used_in": "exBERT: A Visual Analysis Tool to Explore Learned Representations",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1910.05276",
          "note": "Presents exBERT interactive tool."
        }
      ],
      "typical_cost": "medium", 
      "implementation": "https://github.com/bhoov/exbert"
    },
    {
      "name": "Sparse Autoencoders (SAELens + Neuronpedia)",
      "plugin_id": "sae_feature_explorer",
      "task_input": ["generation", "seq2seq", "general_NLP"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general"]},
      "target_scope": "both",
      "description": {
        "overview": "Sparse Autoencoders (SAEs) train a larger, sparse latent space to decompose transformer activations into features. Each sparse unit aims to capture a semantically meaningful concept【60†L23-L32】.",
        "main_functionalities": [
          "Train a sparse overcomplete dictionary on hidden activations",
          "Produce feature activations that reconstruct the original representation",
          "Enable analysis (visualization, ablation) of discovered features"
        ]
      },
      "strengths": [
        "Unsupervised discovery of latent feature directions",
        "Often yields more monosemantic features than raw neurons【60†L23-L32】",
        "Integrates with any model layer for interpretability"
      ],
      "limitations": [
        "Feature meanings must be interpreted (e.g., via top activations)",
        "No guarantee of true monosemantics; polysemantic features may remain",
        "Trade-off between reconstruction quality and sparsity"
      ],
      "research_applications": [
        {
          "used_in": "Towards Monosemanticity: Decomposing LMs with Dictionary Learning",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv (Anthropic)",
          "url": "https://arxiv.org/abs/2306.09194",
          "note": "Introduces large-scale SAEs for discovering monosemantic features."
        },
        {
          "used_in": "Toy Models of Superposition",
          "type": "paper",
          "year": 2022,
          "source": "Transformer Circuits blog",
          "url": "https://transformer-circuits.pub/2022/toy_model/index.html",
          "note": "Explains the superposition hypothesis and role of sparse features."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Information Flow Routes",
      "plugin_id": "meta_transparency_graph",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general"]},
      "target_scope": "both",
      "description": {
        "overview": "Information Flow Routes identifies a sparse computational subgraph that explains a model prediction. It computes attributions for intermediate components and prunes low-contribution edges to reveal key information pathways【60†L23-L32】.",
        "main_functionalities": [
          "Compute component contributions in one forward pass",
          "Build a directed graph of important residual-stream pathways",
          "Extract prediction-specific circuits without costly interventions"
        ]
      },
      "strengths": [
        "Scalable: single-pass attribution finds circuits efficiently【60†L23-L32】",
        "No need for custom corruption or extensive patching",
        "Highlights likely critical components for a prediction"
      ],
      "limitations": [
        "Correlational: not strictly causal",
        "Results depend on thresholding choices and greedy pruning",
        "Might miss alternate pathways or subtle effects"
      ],
      "research_applications": [
        {
          "used_in": "Information Flow Routes: Interpreting LMs at Scale",
          "type": "paper",
          "year": 2024,
          "source": "EMNLP",
          "url": "https://arxiv.org/abs/2404.07047",
          "note": "Introduces the Information Flow Routes technique."
        },
        {
          "used_in": "LM Transparency Tool (ACL 2024 Demo)",
          "type": "paper",
          "year": 2024,
          "source": "ACL Demo",
          "url": "https://aclanthology.org/2024.acl-demos.6.pdf",
          "note": "Presents a user interface leveraging Information Flow Routes."
        }
      ],
      "typical_cost": "medium"
    }, 
    {
  "name": "Integrated Gradients (IG)",
  "plugin_id": "inseq_decoder_ig",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder"]
  },
  "accessibility":"non experts", 
  "target_scope": "local",
  "description": {
    "overview": "Integrated Gradients (IG) attributes a decoder-only language model’s generated tokens to its input tokens by integrating gradients along a path from a baseline to the actual input. Implemented via Inseq, it produces token-level attribution scores explaining next-token predictions.",
    "main_functionalities": [
      "Compute token-level importance scores for generated tokens",
      "Support configurable baselines and integration steps",
      "Visualize attribution scores as interactive HTML heatmaps",
      "Work with Hugging Face-compatible decoder-only models (e.g., GPT-style models)"
    ]
  },
  "strengths": [
    "Faithful gradient-based attribution satisfying sensitivity and implementation invariance",
    "Prediction-specific explanations (local interpretability)",
    "More stable than raw gradient saliency",
    "Integrates seamlessly with generation workflows"
  ],
  "limitations": [
    "Requires white-box access to model gradients",
    "Choice of baseline can significantly affect results",
    "Computationally expensive for long sequences or many integration steps",
    "Attributions remain correlational, not strictly causal"
  ],
  "research_applications": [
    {
      "used_in": "Axiomatic Attribution for Deep Networks",
      "type": "paper",
      "year": 2017,
      "source": "ICML",
      "url": "https://arxiv.org/abs/1703.01365",
      "note": "Introduces Integrated Gradients."
    },
    {
      "used_in": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
      "type": "paper",
      "year": 2023,
      "source": "ACL Demo",
      "url": "https://aclanthology.org/2023.acl-demo.40/",
      "note": "Presents Inseq for attribution in generative language models which implements the Integrated Gradients method for sequence to sequence."
    }
  ],
  "typical_cost": "medium-high"
},
{
  "name": "Integrated Gradients (IG)",
  "plugin_id": "inseq_encdec_ig",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["mixed"],
    "arch": ["encdec"]
  },
  "accessibility":"non experts", 
  "target_scope": "local",
  "description": {
    "overview": "Integrated Gradients (IG) for encoder–decoder models attributes generated target tokens to both source input tokens and decoder states. Using Inseq, it provides fine-grained token-level explanations for sequence-to-sequence predictions such as translation or summarization.",
    "main_functionalities": [
      "Attribute decoder outputs to encoder inputs and decoder context",
      "Support encoder–decoder architectures (e.g., T5, BART)",
      "Provide token-level attribution matrices",
      "Enable visualization of cross-sequence attribution patterns"
    ]
  },
  "strengths": [
    "Captures cross-attention contributions between source and target sequences",
    "Faithful gradient-based attribution method",
    "Applicable to diverse seq2seq tasks (translation, summarization, QA)",
    "Interactive visualization support via Inseq"
  ],
  "limitations": [
    "Higher computational cost due to encoder + decoder passes",
    "Baseline choice affects attribution quality",
    "Requires gradient access (white-box or partial white-box)",
    "Interpretation can be harder due to bidirectional information flow"
  ],
  "research_applications": [
    {
      "used_in": "A Unified Framework for Gradient-based Attribution Methods",
      "type": "paper",
      "year": 2017,
      "source": "ICML",
      "url": "https://arxiv.org/abs/1703.01365",
      "note": "Introduces Integrated Gradients."
    },
    {
      "used_in": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
      "type": "paper",
      "year": 2023,
      "source": "ACL Demo",
      "url": "https://aclanthology.org/2023.acl-demo.40/",
      "note": "Demonstrates attribution methods for encoder–decoder models."
    }
  ],
  "typical_cost": "high"
},
{
  "name": "Discretized Integrated Gradients",
  "plugin_id": "inseq_decoder_discretized_ig",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder"]
  },
  "accessibility":"non experts",
  "target_scope": "local",
  "description": {
    "overview": "Discretized Integrated Gradients extends Integrated Gradients to account for the discrete nature of language model embeddings.",
    "main_functionalities": [
      "Token-level attribution"
    ]
  },
  "strengths": [
    "Like IG, more stable than raw gradients",
    "Satisfies three desirable axioms: completeness, sensitivity and implementation invariance",
    "Tailored to natural language"
  ],
  "limitations": [
    "Baseline choice influences results",
    "Computational cost scales with integration steps"
  ],
   "research_applications": [
    {
      "used_in": "Discretized Integrated Gradients for Explaining Language Models",
      "type": "paper",
      "year": 2021,
      "source": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "url": "https://aclanthology.org/2021.emnlp-main.805.pdf",
      "note": "Introduces Integrated Gradients."
    }
  ],
  "typical_cost": "medium-high"
},
{
  "name": "Discretized Integrated Gradients",
  "plugin_id": "inseq_encdec_discretized_ig",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["encdec"]
  },
  "accessibility":"non experts",
  "target_scope": "local",
  "description": {
    "overview": "Discretized Integrated Gradients extends Integrated Gradients to account for the discrete nature of language model embeddings.",
    "main_functionalities": [
      "Token-level attribution"
        ]
  },
 "strengths": [
    "Like IG, more stable than raw gradients",
    "Satisfies three desirable axioms: completeness, sensitivity and implementation invariance",
    "Tailored to natural language"
  ],
  "limitations": [
    "Baseline choice influences results",
    "Computational cost scales with integration steps"
  ],
  "research_applications": [
    {
      "used_in": "Discretized Integrated Gradients for Explaining Language Models",
      "type": "paper",
      "year": 2021,
      "source": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "url": "https://aclanthology.org/2021.emnlp-main.805.pdf",
      "note": "Introduces Integrated Gradients."
    }
  ],
  "typical_cost": "medium-high"
},
{
  "name": "PCA",
  "plugin_id": "embedding_pca_layers",
  "task_input": ["generation", "classification"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder", "encdec"]
  },
  "target_scope": "local",
  "description": {
    "overview": "Projects token representations (input embeddings and hidden states) into 2D using Principal Component Analysis (PCA) to visualize the geometry of transformer representations across layers. Users can choose a single fixed PCA basis (for cross-layer comparison) or per-layer bases (for layer-local structure).",
    "main_functionalities": [
      "Token-level PCA projections for any model layer",
      "Single-basis mode for cross-layer comparative visualization",
      "Per-layer PCA mode for exploring layer-specific representational structure",
      "Interactive layer selection and labeled scatter plots of tokens in PCA space"
    ]
  },
  "strengths": [
    "Provides intuitive visualization of representational geometry in high-dimensional spaces",
    "Reveals layerwise evolution of token representations",
    "Supports both fixed and per-layer PCA bases for flexible analysis"
  ],
  "limitations": [
    "PCA captures only linear structure and may miss non-linear geometry",
    "PCA of high-dimensional token embeddings misses a lot of important information and may destroy important semantic relationship",
    "Visualization quality depends on token count and sentence length"
  ],
  "research_applications": [
    {
      "used_in": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction",
      "type": "paper",
      "year": 2025,
      "source": "arXiv: Visualizing LLM Latent Space Geometry Through Dimensionality Reduction",
      "url": "https://arxiv.org/abs/2511.21594",
      "note": "Applies PCA (and UMAP) to analyze latent state geometries and layerwise evolution in transformer models."
    },
    {
      "used_in": "Examining Structure of Word Embeddings with PCA",
      "type": "paper",
      "year": 2019,
      "source": "International Conference of Text, Speech and Dialogue (TSD 2019)",
      "url": "https://arxiv.org/abs/1906.00114",
      "note": "Uses PCA to compare and interpret the geometric structure of word embeddings from different models."
    },
    {
      "used_in": "Learned Transformer Position Embeddings Have a Low-Dimensional Structure",
      "type": "paper",
      "year": 2024,
      "source": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP 2024)",
      "url": "https://aclanthology.org/2024.repl4nlp-1.17.pdf",
      "note": "Applies PCA to analyze the dimensionality and structure of positional and word embeddings in BERT models. They find that positional embeddings have a low-dimensional structure, using only about 10% of the available dimensions."
    }
    ] 
    }, 
{
  "name": "GradientSHAP",
  "plugin_id": "inseq_decoder_gradient_shap",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder"]
  },
  "accessibility":"non experts",
  "target_scope": "local",
  "description": {
    "overview": "GradientSHAP combines Integrated Gradients with noise sampling to approximate Shapley values for decoder-only language models.",
    "main_functionalities": [
      "Monte Carlo sampling over noisy baselines",
      "Approximate Shapley-style token attribution",
      "Token-level importance visualization"
    ]
  },
  "strengths": [
    "Theoretically connected to Shapley values",
    "More robust than single-baseline IG",
    "Handles gradient saturation better than raw gradients"
  ],
  "limitations": [
    "Higher variance due to sampling",
    "Computationally expensive",
    "Requires gradient access"
  ],
  "typical_cost": "high"
},
{
  "name": "DeepLIFT",
  "plugin_id": "inseq_decoder_deeplift",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder"]
  },
  "target_scope": "local",
  "description": {
    "overview": "DeepLIFT attributes output differences to input tokens by comparing activations to a reference baseline and propagating contribution scores backward through the model.",
    "main_functionalities": [
      "Reference-based contribution propagation",
      "Handles gradient saturation better than IG",
      "Token-level attribution scores"
    ]
  },
  "strengths": [
    "Computationally cheaper than multi-step IG",
    "Less sensitive to gradient saturation",
    "Clear reference-based interpretation"
  ],
  "limitations": [
    "Depends strongly on baseline choice",
    "May violate implementation invariance",
    "Requires white-box access"
  ],
  "typical_cost": "medium"
},
{
  "name": "Input × Gradient",
  "plugin_id": "inseq_decoder_input_x_gradient",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder"]
  },
  "target_scope": "local",
  "description": {
    "overview": "Input × Gradient multiplies input embeddings by their gradients to estimate token importance for decoder-only language models.",
    "main_functionalities": [
      "Single backward pass attribution",
      "Token-level importance estimation",
      "Fast computation"
    ]
  },
  "strengths": [
    "Very fast (single gradient pass)",
    "Simple to interpret",
    "Low computational overhead"
  ],
  "limitations": [
    "Noisy and unstable",
    "Sensitive to gradient saturation",
    "Purely local linear approximation"
  ],
  "typical_cost": "low"
}, 
{
  "name": "LIME",
  "plugin_id": "inseq_decoder_lime",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["black_box"],
    "arch": ["decoder"]
  }, 
  "target_scope": "local",
  "accessibility":"non experts",
  "description": {
    "overview": "LIME approximates a local linear surrogate model around a prediction by perturbing input tokens and observing changes in generated outputs.",
    "main_functionalities": [
      "Perturbation-based token attribution",
      "Model-agnostic explanation",
      "Local surrogate modeling"
    ]
  },
  "strengths": [
    "Does not require gradient access",
    "Model-agnostic",
    "Intuitive local linear approximation"
  ],
  "limitations": [
    "Sampling-based and potentially unstable",
    "Computationally expensive for long sequences",
    "Faithfulness depends on perturbation strategy"
  ],
   "research_applications": [
    {
      "used_in": "Why Should I Trust You? Explaining the Predictions of Any Classifier",
      "type": "paper",
      "year": 2016,
      "source": "Proceedings of Knowledge Discovery in Databases (KDD)",
      "url": "https://arxiv.org/abs/1602.04938",
      "note": "Original LIME paper introducing local surrogate explanations."
    },
    {
      "used_in": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
      "type": "paper",
      "year": 2023,
      "source": "ACL Demo",
      "url": "https://aclanthology.org/2023.acl-demo.40/",
      "note": "Demonstrates LIME-style attribution for generative language models."
    }
  ],
  "typical_cost": "high"
}, 
{
  "name": "GradientSHAP",
  "plugin_id": "inseq_encdec_gradient_shap",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["encdec"]
  },
  "accessibility":"non experts",
  "target_scope": "local",
  "description": {
    "overview": "GradientSHAP combines Integrated Gradients with noise sampling to approximate Shapley values for encoder-decoder language models.",
    "main_functionalities": [
      "Monte Carlo sampling over noisy baselines",
      "Approximate Shapley-style token attribution",
      "Token-level importance visualization"
    ]
  },
  "strengths": [
    "Theoretically connected to Shapley values",
    "More robust than single-baseline IG",
    "Handles gradient saturation better than raw gradients"
  ],
  "limitations": [
    "Higher variance due to sampling",
    "Computationally expensive",
    "Requires gradient access"
  ],
  "typical_cost": "high"
},
{
  "name": "DeepLIFT",
  "plugin_id": "inseq_encdec_deeplift",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["encdec"]
  },
  "target_scope": "local",
  "description": {
    "overview": "DeepLIFT attributes output differences to input tokens by comparing activations to a reference baseline and propagating contribution scores backward through the model.",
    "main_functionalities": [
      "Reference-based contribution propagation",
      "Handles gradient saturation better than IG",
      "Token-level attribution scores"
    ]
  },
  "strengths": [
    "Computationally cheaper than multi-step IG",
    "Less sensitive to gradient saturation",
    "Clear reference-based interpretation"
  ],
  "limitations": [
    "Depends strongly on baseline choice",
    "May violate implementation invariance",
    "Requires white-box access"
  ],
  "typical_cost": "medium"
},
{
  "name": "Input × Gradient",
  "plugin_id": "inseq_encdec_input_x_gradient",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["encdec"]
  },
  "target_scope": "local",
  "description": {
    "overview": "Input × Gradient multiplies input embeddings by their gradients to estimate token importance for encoder-decoder language models.",
    "main_functionalities": [
      "Single backward pass attribution",
      "Token-level importance estimation",
      "Fast computation"
    ]
  },
  "strengths": [
    "Very fast (single gradient pass)",
    "Simple to interpret",
    "Low computational overhead"
  ],
  "limitations": [
    "Noisy and unstable",
    "Sensitive to gradient saturation",
    "Purely local linear approximation"
  ],
  "typical_cost": "low"
}, 
{
  "name": "LIME",
  "plugin_id": "inseq_encdec_lime",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["black_box"],
    "arch": ["encdec"]
  },
  "target_scope": "local",
  "accessibility":"non experts",
  "description": {
    "overview": "LIME approximates a local linear surrogate model around a prediction by perturbing input tokens and observing changes in generated outputs.",
    "main_functionalities": [
      "Perturbation-based token attribution",
      "Model-agnostic explanation",
      "Local surrogate modeling"
    ]
  },
  "strengths": [
    "Does not require gradient access",
    "Model-agnostic",
    "Intuitive local linear approximation"
  ],
  "limitations": [
    "Sampling-based and potentially unstable",
    "Computationally expensive for long sequences",
    "Faithfulness depends on perturbation strategy"
  ],
  "typical_cost": "high"
   }, 
    {
  "name": "Linear CKA (layer similarity)",
  "plugin_id": "linear_cka_layers",
  "notes": "Representation similarity across layers using linear CKA.",
  "task_input": ["general_NLP", "classification", "generation"],
  "access_arch": { "access": "white_box", "arch": "transformer_general" },
  "target_scope": "global",
  "granularity": ["layer"],
  "user_goal_audience": ["research_debug", "mech_interp", "model_eval"],
  "fidelity": "mixed",
  "format": ["visual_UI", "metrics"],
  "description": {
    "overview": "Computes a layer-by-layer linear CKA matrix to visualize representation similarity across transformer layers.",
    "main_functionalities": [
      "Extract hidden states for each layer",
      "Compute linear CKA similarity matrix",
      "Visualize similarities as a heatmap"
    ]
  },
  "strengths": [
    "Good for comparing how representations evolve across depth",
    "Stable summary metric (compared to raw cosine on single tokens)"
  ],
  "limitations": [
    "Single-input estimate can be noisy; best with multiple inputs",
    "CKA depends on token sampling and preprocessing choices"
  ],
  "research_applications": [
    {
      "used_in": "Similarity of Neural Network Representations Revisited",
      "type": "paper",
      "year": 2019,
      "source": "Proceedings of the 36 th International Conference on Machine Learning (ICML 2019)",
      "url": "https://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf",
      "note": "Introduces the linear CKA measure for comparing neural network representations."
    }, 
    {
    "used_in": "Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers",
    "type": "paper",
    "year": 2021,
    "source": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    "url": "https://aclanthology.org/2021.blackboxnlp-1.42.pdf",
    "note": "Uses Centered Kernel Alignment (CKA) to measure representational similarity across layers in fine-tuned transformer models (e.g., RoBERTa, ALBERT)."
  }, 
  {
  "used_in": "Ecco: An Open Source Library for the Explainability of Transformer Language Models",
  "type": "paper",
  "year": 2021,
  "source": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
  "url": "https://aclanthology.org/2021.acl-demo.30.pdf",
  "note": "Introduces the Ecco toolkit for analyzing transformer language models. Includes implementation of CKA for comparing layer representations and studying representational similarity across model depth."
}
  ]
}, 
 {
  "name": "Non-negative Matrix Factorization (NMF)",
  "task_input": [ "classification", "generation"],
  "access_arch": { "access": "white_box", "arch": "all" },
  "accessibility": "experts", 
  "description": {
    "overview": "Matrix factorization approaches such as non-negative matrix factorization decompose activations into a concise set of patterns.",
    "main_functionalities": [
      "Summarizes activation with a small number of factors",
      "Factors can be intepreted much more easily than individual neurons"
      ]
  },
  "strengths": [
    "Unlike several alternative matrix-factorization approaches, NMF enforces non-negativity, thus avoiding cancellation effects and highlighting semantic clusters", 
    "Fast algorithms to compute NMF exist"
  ],
  "limitations": [
    "Enforcing non-negative can result in information loss", 
    "Assumes an additive structure for activations which may not hold in practice"
  ],
  "research_applications": [ 
  {
  "used_in": "Ecco: An Open Source Library for the Explainability of Transformer Language Models",
  "type": "paper",
  "year": 2021,
  "source": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
  "url": "https://aclanthology.org/2021.acl-demo.30.pdf",
  "note": "Introduces the Ecco toolkit for analyzing transformer language models. Includes implementation of CKA for comparing layer representations and studying representational similarity across model depth."
}, 
  {
  "used_in": "Algorithms for Non-negative Matrix Factorization",
  "type": "paper",
  "year": 2000,
  "source": "Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference",
  "url": "https://proceedings.neurips.cc/paper_files/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf",
  "note": "Introduces multiplicative update rules to compute NMF"
}, 
  {
  "used_in": "Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization",
  "type": "paper",
  "year": 2025,
  "source": "ArXiv",
  "url": "https://arxiv.org/pdf/2506.10920",
  "note": "Proposes decomposing MLP activations with Semi-Nonnegative Matrix Factorization for finding interpretable features in the activations. "
}
  ], 
  "implementation": "https://github.com/jalammar/ecco"
},
{
  "name": "CCA (layer similarity)",
  "plugin_id": "cca_layers",
  "notes": "Representation similarity across transformer layers using Canonical Correlation Analysis (CCA) via SVCCA-lib.",
  "task_input": ["classification", "generation"],
  "access_arch": { "access": "white_box", "arch": "transformer_general" },
  "target_scope": "global",
  "description": {
    "overview": "Computes a layer-by-layer similarity matrix using Canonical Correlation Analysis (CCA) to compare hidden representations across transformer layers.",
    "main_functionalities": [
      "Extract hidden states for each transformer layer",
      "Compute mean canonical correlation between layer representations",
      "Visualize layer similarity matrix as a heatmap"
    ]
  },
  "strengths": [
    "Invariant to linear transformations of representation space",
    "Widely used in representation analysis research"
  ],
  "limitations": [
    "Can be numerically unstable without regularization"
  ],
  "research_applications": [
    {
      "used_in": "Canonical Correlation Analysis",
      "type": "paper",
      "year": 1936,
      "source": "Biometrika",
      "url": "https://www.jstor.org/stable/2333955",
      "note": "Original introduction of Canonical Correlation Analysis by Hotelling."
    },
    {
      "used_in": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
      "type": "paper",
      "year": 2017,
      "source": "NeurIPS 2017",
      "url": "https://arxiv.org/abs/1706.05806",
      "note": "Introduces SVCCA for analyzing representations in deep neural networks; foundational for representation similarity analysis in modern deep models."
    },
    {
      "used_in": "Insights on Representational Similarity in Neural Networks with Canonical Correlation",
      "type": "paper",
      "year": 2018,
      "source": "NeurIPS 2018",
      "url": "https://arxiv.org/abs/1806.05759",
      "note": "Introduces Projection-Weighted CCA (PWCCA) and further analyzes representation similarity in deep networks."
    },
    {
      "used_in": "BERT is Not an Interlingua and the Bias of Tokenization",
      "type": "paper",
      "year": 2019,
      "source": "ACL 2019",
      "url": "https://aclanthology.org/D19-6106.pdf",
      "note": "Uses Canonical Correlation Analysis (CCA) to find that pretrained multilingual BERT does not learn a unified interlingual representation space. Instead, it partitions its internal representations by language."
    },
    {
      "used_in": "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives",
      "type": "paper",
      "year": 2019,
      "source": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "url": "https://aclanthology.org/D19-1448.pdf",
      "note": "Uses canonical correlation analysis to investigate how information flows across transformer layers and how the flow hinges on the objective adopted for learning."
    }, 
    {
      "used_in": "Similarity Analysis of Contextual Word Representation Models",
      "type": "paper",
      "year": 2020,
      "source": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "url": "https://aclanthology.org/2020.acl-main.422.pdf",
      "note": "Uses canonical correlation analysis to measure the similarity between different latent representations."
    },
    {
      "used_in": "Ecco: An Open Source Library for the Explainability of Transformer Language Models",
      "type": "paper",
      "year": 2021,
      "source": "ACL 2021 System Demonstrations",
      "url": "https://aclanthology.org/2021.acl-demo.30.pdf",
      "note": "Introduces the Ecco toolkit for analyzing transformer language models. Implements CCA and SVCCA to study representational similarity across layers."
    }
  ]
}
  ]
}
