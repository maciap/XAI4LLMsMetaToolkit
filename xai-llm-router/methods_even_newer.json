{
  "methods": [
    {
      "name": "Integrated Gradients (Captum)",
      "plugin_id": "captum_ig_classifier",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Integrated Gradients (IG) attributes a model’s prediction to input tokens by integrating gradients along a path from a baseline to the actual input. It produces per-token attributions for the target output.",
        "main_functionalities": [
          "Compute token-level importance by integrating gradients from a baseline",
          "Support configurable number of integration steps",
          "Applicable to any differentiable model (white-box access required)"
        ]
      },
      "strengths": [
        "Satisfies axioms of sensitivity and implementation invariance【60†L23-L32】",
        "More stable and less noisy than raw gradients",
        "Provides intuitive baseline comparison ('change from reference to input')"
      ],
      "limitations": [
        "Requires choosing a baseline, which affects results【60†L23-L32】",
        "Computationally expensive due to multiple forward/backward passes",
        "Correlational attribution (not strictly causal without interventions)"
      ],
      "research_applications": [
        {
          "used_in": "Axiomatic Attribution for Deep Networks",
          "type": "paper",
          "year": 2017,
          "source": "ICML",
          "url": "https://dl.acm.org/doi/10.5555/3305890.3306024",
          "note": "Introduces the Integrated Gradients method."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/integrated_gradients.html",
          "note": "Details the Integrated Gradients implementation in Captum."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Layer Integrated Gradients (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Layer Integrated Gradients applies the IG approach to internal model layers instead of inputs. It attributes importance to activations of a chosen intermediate layer.",
        "main_functionalities": [
          "Compute attributions for internal layer inputs or outputs",
          "Enable analysis of intermediate representations"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/layer_integrated_gradients.html",
          "note": "Documents the layer-wise IG method in Captum."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Saliency (Captum)",
      "plugin_id": "captum_saliency_classifier",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["transformer_general", "encoder", "NA"]},
      "target_scope": "local",
      "description": {
        "overview": "Saliency computes the gradient of the output score with respect to each input embedding, producing a token-level importance heatmap. It highlights tokens whose small changes most affect the output.",
        "main_functionalities": [
          "Compute token importance as the magnitude of input gradients",
          "Fast single-pass computation (one forward and backward)",
          "No baseline required"
        ]
      },
      "strengths": [
        "Very fast (single backward pass)",
        "Easy to interpret as 'which tokens influence the output most'",
        "Useful for quick debugging and visualization"
      ],
      "limitations": [
        "Often noisy or unstable, highlighting spurious tokens",
        "Suffers from gradient saturation (zero gradients) and discontinuities",
        "Sensitivity scores do not guarantee causal importance"
      ],
      "research_applications": [
        {
          "used_in": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
          "type": "paper",
          "year": 2014,
          "source": "ICLR Workshop",
          "url": "https://arxiv.org/pdf/1312.6034",
          "note": "Introduces the basic saliency/gradient method."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/saliency.html",
          "note": "Describes Saliency in Captum."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "Input\u00d7Gradient (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Input\u00d7Gradient multiplies each input embedding by its gradient, yielding signed attributions. It highlights features with large values and gradients.",
        "main_functionalities": [
          "Compute contributions by combining input values with gradients"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/input_x_gradient.html",
          "note": "Details the Input\u00d7Gradient method in Captum."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "DeepLIFT (Captum)",
      "plugin_id": "captum_deeplift_classifier",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["transformer_general", "encoder", "NA"]},
      "target_scope": "local",
      "description": {
        "overview": "DeepLIFT attributes a change in the output to changes in input relative to a baseline. It backpropagates differences in activation to compute contributions of each input.",
        "main_functionalities": [
          "Compute baseline-based token attributions",
          "Handle gradient saturation by non-linear backpropagation rules"
        ]
      },
      "strengths": [
        "Often faster than high-step IG (fewer passes)",
        "Handles cases of zero gradients (saturation) better than raw gradients",
        "Produces sharper attributions than Saliency"
      ],
      "limitations": [
        "Baseline choice still affects results (like IG)",
        "Resulting attribution depends on network structure and non-linearities",
        "Correlational attribution (not inherently causal)"
      ],
      "research_applications": [
        {
          "used_in": "Learning Important Features Through Propagating Activation Differences",
          "type": "paper",
          "year": 2017,
          "source": "ICML",
          "url": "https://proceedings.mlr.press/v70/shrikumar17a.html",
          "note": "Introduces DeepLIFT method."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/deeplift.html",
          "note": "Describes DeepLIFT in Captum."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "GradientSHAP (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "GradientSHAP combines integrated gradients with randomized baselines. It approximates Shapley values by averaging integrated gradients from multiple noisy baseline inputs.",
        "main_functionalities": [
          "Sample multiple baselines and integrate gradients",
          "Approximate Shapley-value attributions via gradient estimates"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP framework combining kernel and gradient methods."
        },
        {
          "used_in": "Captum Documentation (GitHub)",
          "type": "docs",
          "year": 2023,
          "source": "Facebook AI Research",
          "url": "https://captum.ai/api/gradient_shap.html",
          "note": "Describes GradientSHAP in Captum."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Feature Ablation (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Feature Ablation is a perturbation method that measures output change when input features (tokens) are removed or replaced with a baseline. It estimates importance by deletion impact.",
        "main_functionalities": [
          "Remove or mask input tokens and observe output differences",
          "Compute importance scores as change in prediction"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Global Explanation of Neural Networks via Feature Ablation",
          "type": "paper",
          "year": 2019,
          "source": "ArXiv",
          "url": "https://arxiv.org/abs/1806.03824",
          "note": "Discusses feature ablation methods for model interpretability."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Occlusion (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Occlusion masks out contiguous input segments (e.g., spans of tokens) and measures the effect on model output. The difference indicates the importance of the occluded region.",
        "main_functionalities": [
          "Systematically mask segments of input text",
          "Measure output change to infer importance of segments"
        ]
      },
      "strengths": [
        "Intuitive and model-agnostic (no gradients needed)",
        "Highlights contiguous spans that influence output"
      ],
      "limitations": [
        "Requires many forward passes (computationally expensive)",
        "Masked inputs (e.g., with UNK) can be unnatural",
        "Local method only (does not summarize global behavior)"
      ],
      "research_applications": [
        {
          "used_in": "Visualizing and Understanding Convolutional Networks",
          "type": "paper",
          "year": 2014,
          "source": "ECCV",
          "url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53",
          "note": "Popularizes occlusion analysis for CNNs."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Kernel SHAP (InterpretML: ShapKernel)",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "all",
      "description": {
        "overview": "Kernel SHAP is a model-agnostic method to estimate feature attributions by approximating Shapley values with a weighted linear regression on perturbations.",
        "main_functionalities": [
          "Compute local Shapley-based attributions for a predictor",
          "Aggregate local explanations for a global summary (if needed)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP (Lundberg & Lee, 2017)."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "LIME (InterpretML: LimeTabular)",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "LIME (Local Interpretable Model-agnostic Explanations) fits a simple surrogate model around a specific instance by sampling perturbations. The surrogate’s weights on features explain the local prediction.",
        "main_functionalities": [
          "Generate local explanations by fitting an interpretable model (e.g., linear) around an instance",
          "Learn feature weights from perturbed samples"
        ]
      },
      "strengths": [
        "Provides interpretable local explanations via simple models",
        "Model-agnostic and widely applicable"
      ],
      "limitations": [
        "Requires many perturbed samples (computationally heavy)",
        "Explanations sensitive to sampling and kernel choices"
      ],
      "research_applications": [
        {
          "used_in": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
          "type": "paper",
          "year": 2016,
          "source": "KDD",
          "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
          "note": "Introduces the LIME method."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Partial Dependence Plot (InterpretML: PartialDependence)",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "global",
      "description": {
        "overview": "Partial Dependence Plots (PDP) estimate the marginal effect of one or more features on the predicted outcome by averaging model outputs over a dataset.",
        "main_functionalities": [
          "Compute average model predictions for varying feature values",
          "Visualize global feature–prediction relationships"
        ]
      },
      "strengths": [
        "Provides a global view of feature effects",
        "Easy to interpret trends of feature influence"
      ],
      "limitations": [
        "Assumes feature independence; can mislead if features correlate",
        "Only captures marginal (average) effects, not interactions"
      ],
      "research_applications": [
        {
          "used_in": "Greedy Function Approximation: A Gradient Boosting Machine (Friedman)",
          "type": "paper",
          "year": 2001,
          "source": "Annals of Statistics",
          "url": "https://projecteuclid.org/euclid.aos/1013203451",
          "note": "Introduces partial dependence plots."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "Morris Sensitivity Analysis (InterpretML: MorrisSensitivity)",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "global",
      "description": {
        "overview": "Morris Sensitivity analysis is a global sensitivity method that samples feature space along trajectories and computes elementary effects. It estimates each feature’s influence on the output variance.",
        "main_functionalities": [
          "Compute Morris 'elementary effects' by varying one feature at a time",
          "Estimate global sensitivity indices for features"
        ]
      },
      "strengths": [
        "Model-agnostic global feature importance",
        "Requires fewer samples than full factorial designs"
      ],
      "limitations": [
        "Computationally expensive in high dimensions",
        "Results depend on trajectory design and sampling"
      ],
      "research_applications": [
        {
          "used_in": "Factorial Sampling Plans for Preliminary Computational Experiments (Morris)",
          "type": "paper",
          "year": 1991,
          "source": "Technometrics",
          "url": "https://doi.org/10.1080/00401706.1991.10484804",
          "note": "Introduces Morris sensitivity method."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Anchors",
      "plugin_id": "alibi_anchors_text",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["transformer_general"]},
      "target_scope": "local",
      "description": {
        "overview": "Anchors identifies a set of words or feature conditions (an 'anchor') that fix a model’s prediction locally. It generates IF-THEN rules guaranteeing the prediction with high precision.",
        "main_functionalities": [
          "Find words/phrases whose presence anchors the prediction",
          "Provide precision (reliability) and coverage metrics for anchors",
          "Show example inputs where the anchor applies or fails"
        ]
      },
      "strengths": [
        "Produces intuitive IF-THEN rule explanations",
        "Model-agnostic: works with any classifier",
        "Quantifies explanation reliability and scope"
      ],
      "limitations": [
        "Local explanation only (no global insight)",
        "Computationally expensive for long texts (sampling large neighborhoods)",
        "Quality depends on perturbation strategy (masking may be unnatural)"
      ],
      "research_applications": [
        {
          "used_in": "Anchors: High-Precision Model-Agnostic Explanations",
          "type": "paper",
          "year": 2018,
          "source": "AAAI",
          "url": "https://homes.cs.washington.edu/~marcotcr/aaai18.pdf",
          "note": "Original paper introducing Anchors."
        },
        {
          "used_in": "A Sea of Words: Analysis of Anchors for Text",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2205.13789",
          "note": "Analyzes theoretical properties of Anchors for text classification."
        },
        {
          "used_in": "Alibi Explain: Model Explanation Library",
          "type": "paper",
          "year": 2021,
          "source": "JMLR",
          "url": "https://jmlr.org/papers/v22/21-0017.html",
          "note": "Describes Alibi library, which implements Anchors."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Direct Logit Attribution",
      "plugin_id": "direct_logit_attribution",
      "task_input": ["generation", "seq2seq", "general_NLP", "NA"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general", "NA"]},
      "target_scope": "local",
      "description": {
        "overview": "Direct Logit Attribution (DLA) projects each model component’s contribution onto the target token’s logit in the unembedding space. Positive contributions push the logit up (toward the token), negatives push it down.",
        "main_functionalities": [
          "Project each layer, attention head, or MLP output onto the target token’s output weight",
          "Compute signed contributions of components to a chosen token’s logit",
          "Rank components by their absolute contribution magnitude"
        ]
      },
      "strengths": [
        "Fast and simple: one forward pass plus linear projections",
        "Intuitive signed contributions (direct evidence vs. counter-evidence)",
        "Helps locate where the prediction is being written in the model"
      ],
      "limitations": [
        "Not causal: large contribution might be overridden later",
        "Ignores interactions (softmax competition, nonlinear effects)",
        "Depends on normalization placement (affects interpretability)",
        "Can mislead if components clean up earlier signals"
      ],
      "research_applications": [
        {
          "used_in": "Attribution Patching: Activation Patching at Industrial Scale",
          "type": "blog",
          "year": 2023,
          "source": "Neel Nanda blog",
          "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching",
          "note": "Discusses Direct Logit Attribution as a diagnostic method."
        },
        {
          "used_in": "TransformerLens ActivationCache.decompose_resid",
          "type": "docs",
          "year": 2023,
          "source": "TransformerLens",
          "url": "https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.ActivationCache.html",
          "note": "Tool support for decomposing residual streams, enabling DLA."
        },
        {
          "used_in": "A Mathematical Framework for Transformer Circuits",
          "type": "article",
          "year": 2021,
          "source": "Transformer Circuits blog",
          "url": "https://transformer-circuits.pub/2021/framework/index.html",
          "note": "Presents the residual stream view motivating attribution approaches."
        },
        {
          "used_in": "Adversarial Example for Direct Logit Attribution",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2310.07325",
          "note": "Demonstrates cases where DLA can be misleading (residual erasure)."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "Logit Lens",
      "plugin_id": "logit_lens",
      "task_input": ["generation", "seq2seq", "general_NLP", "NA"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general", "NA"]},
      "target_scope": "local",
      "description": {
        "overview": "Logit Lens projects each intermediate hidden state through the model’s output head to view predicted token probabilities at each layer. It tracks how the model’s top prediction evolves through the layers【64†L53-L61】.",
        "main_functionalities": [
          "Compute top-k token predictions for each layer at a chosen position",
          "Track the target token’s logit or probability across layers",
          "Compare layer-wise predictions under optional normalization"
        ]
      },
      "strengths": [
        "Very fast: one forward pass provides layer-wise outputs",
        "Shows when and how representations become linearly decodable",
        "Natural for decoder-only LMs (no additional sampling)"
      ],
      "limitations": [
        "Not causal: reveals decode-ability, not true necessity",
        "Intermediate states require proper normalization for comparison",
        "Top-k outputs can be misleading when probabilities are diffuse"
      ],
      "research_applications": [
        {
          "used_in": "Interpreting GPT: The Logit Lens",
          "type": "blog",
          "year": 2020,
          "source": "LessWrong",
          "url": "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens",
          "note": "Introduces the logit lens idea for GPT."
        },
        {
          "used_in": "Information Flow of Transformers (EMNLP 2023 Findings)",
          "type": "paper",
          "year": 2023,
          "source": "ACL Anthology",
          "url": "https://aclanthology.org/2023.findings-emnlp.939.pdf",
          "note": "Applies logit lens to study information flow."
        },
        {
          "used_in": "What Language Do Non-English LLMs Think In? (ACL 2025 Findings)",
          "type": "paper",
          "year": 2025,
          "source": "ACL Anthology",
          "url": "https://aclanthology.org/2025.findings-acl.1350.pdf",
          "note": "Uses logit lens to detect latent language signals."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "Layer-wise Relevance Propagation (LRP)",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "LRP redistributes the prediction score backward through the network to the inputs, assigning relevance scores via fixed propagation rules. It accounts for network structure (including positional encodings in Transformers) to conserve relevance【64†L53-L61】.",
        "main_functionalities": [
          "Backpropagate a target score to input features through relevance rules",
          "Ensure relevance conservation (inputs sum to output)",
          "Handle transformer components like positional embeddings specially"
        ]
      },
      "strengths": [
        "Theoretically grounded propagation rules enforce conservation【64†L53-L61】",
        "Designed to handle nonlinearities and structural layers",
        "Applicable to vision and NLP transformer models"
      ],
      "limitations": [
        "Complex to implement (requires layer-specific rules)【64†L53-L61】",
        "Results depend on chosen LRP rule set",
        "Needs adaptation for different model variants"
      ],
      "research_applications": [
        {
          "used_in": "Revisiting LRP: Positional Attribution for Transformers",
          "type": "paper",
          "year": 2025,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2506.02138",
          "note": "Adaptation of LRP to Transformer models, accounting for positional encodings."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "Attention Rollout",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "Attention Rollout aggregates multi-head self-attention across layers by multiplying attention matrices. It computes how strongly each input token contributes to an output token through the attention pathway.",
        "main_functionalities": [
          "Aggregate attention weights across heads and layers (rollout)",
          "Compute token-to-token relevance scores via attention product",
          "Visualize cumulative attention flow from inputs to outputs"
        ]
      },
      "strengths": [
        "Efficient: uses existing attention weights (no backprop required)",
        "Provides intuitive visualization of attention dependencies"
      ],
      "limitations": [
        "Not truly causal (ignores non-attention parts of the model)",
        "Effectiveness depends on meaningful attention (not all heads are informative)"
      ],
      "research_applications": [
        {
          "used_in": "Quantifying Attention Flow in Transformers",
          "type": "paper",
          "year": 2020,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2005.00928",
          "note": "Introduces the Attention Rollout method."
        }
      ],
      "typical_cost": "low"
    },
    {
      "name": "PnPXAI Kernel SHAP",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "all",
      "description": {
        "overview": "Kernel SHAP (in PnPXAI) is a model-agnostic Shapley-value estimator, using a weighted linear regression on perturbations. It provides local explanations similarly to general Kernel SHAP.",
        "main_functionalities": [
          "Compute local Shapley explanations for model predictions"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "PnPXAI: A Universal XAI Framework",
          "type": "paper",
          "year": 2025,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2505.10515",
          "note": "Describes PnPXAI framework, including Kernel SHAP."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "DALEX BreakDown",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "BreakDown (in DALEX) decomposes a model’s prediction for a single instance into additive contributions of each feature along an ordered path.",
        "main_functionalities": [
          "Compute per-feature contributions for one prediction",
          "Enable interactive investigation of contribution order"
        ]
      },
      "strengths": [
        "Gives clear additive feature contributions",
        "Local explanation with simple interpretation"
      ],
      "limitations": [
        "Depends on feature ordering (non-unique explanations)",
        "May not capture feature interactions"
      ],
      "research_applications": [],
      "typical_cost": "medium"
    },
    {
      "name": "DALEX Shap (random-path Shapley values)",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "DALEX Shap approximates Shapley values by averaging feature contributions across random permutations (paths). It provides local Shapley-based explanations.",
        "main_functionalities": [
          "Compute Shapley-style attributions via random feature orderings",
          "Provides local explanations that sum to output change"
        ]
      },
      "strengths": [
        "Grounded in cooperative game theory (Shapley values)",
        "Local feature attributions summing to the prediction"
      ],
      "limitations": [
        "Computationally expensive with many features",
        "Sampling variance in attributions"
      ],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
          "note": "Introduces SHAP framework (Lundberg & Lee, 2017)."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "FAT Forensics CounterfactualExplainer",
      "task_input": ["classification"],
      "access_arch": {"access": ["black_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "CounterfactualExplainer generates counterfactual instances that change a black-box model’s prediction. These are minimal edits to the input to flip the predicted class.",
        "main_functionalities": [
          "Generate counterfactual examples for a given prediction",
          "Report the differences needed to alter the model decision"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "FAT Forensics: Fairness, Accountability and Transparency Toolbox",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1909.05167",
          "note": "Describes the FAT Forensics toolbox including counterfactual modules."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Activation Patching (TransformerLens)",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "Activation Patching compares two model runs by copying ('patching') certain internal activations from one run to another. The change in output reveals the causal effect of those activations.",
        "main_functionalities": [
          "Replace specified activations from a 'clean' run into a 'corrupted' run",
          "Measure output differences to identify critical components"
        ]
      },
      "strengths": [
        "Causal: directly tests necessity of components",
        "Allows pinpointing precise layers/neurons affecting a behavior"
      ],
      "limitations": [
        "Computationally expensive (requires many runs)",
        "Results depend on choice of baseline and interventions"
      ],
      "research_applications": [
        {
          "used_in": "How to Use and Interpret Activation Patching",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2404.15255",
          "note": "Provides methodology and best practices for activation patching."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Path Patching",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "Path Patching is a variant where patches are constrained to a single component's output, testing direct vs. indirect influence. It assesses whether two components have a direct causal connection.",
        "main_functionalities": [
          "Patch activations while limiting effect to one target component",
          "Distinguish direct from mediated causal pathways"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "How to Use and Interpret Activation Patching",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2404.15255",
          "note": "Introduces path-constrained activation patching."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Attribution Patching",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "Attribution Patching approximates activation patching using gradient-based calculations. It estimates many patching effects efficiently by propagating attribution scores.",
        "main_functionalities": [
          "Use gradients to approximate the effect of multiple patches",
          "Enable large-scale causal analysis with fewer runs"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Attribution Patching: Activation Patching at Industrial Scale",
          "type": "blog",
          "year": 2026,
          "source": "Neel Nanda blog",
          "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching",
          "note": "Introduces attribution patching."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "VISIT semantic information flow graph",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "VISIT visualizes a forward pass as a semantic flow graph. Nodes represent neurons or activations and edges represent interactions, projected into vocabulary space to trace information flow.",
        "main_functionalities": [
          "Construct interactive flow graph of activations and attentions",
          "Enable analysis of attention heads and memory values via vocabulary projections"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "VISIT: Visualizing Semantic Information Flow of Transformers",
          "type": "paper",
          "year": 2023,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2305.13417",
          "note": "Introduces the VISIT flow-graph visualization tool."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "Vocabulary-space FFN update decomposition",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder"]},
      "target_scope": "local",
      "description": {
        "overview": "This method decomposes a Transformer's feed-forward (MLP) output as updates in vocabulary token space. Each part corresponds to parameter vectors promoting specific word concepts.",
        "main_functionalities": [
          "Project FFN output changes into vocabulary logit space",
          "Decompose the update into components tied to learned concepts"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Transformer FFN layers build predictions by promoting vocabulary-space concepts",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2203.14680",
          "note": "Introduces vocabulary-space FFN decomposition."
        },
        {
          "used_in": "LM-Debugger: Inspection and Intervention tool",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2204.12130",
          "note": "Uses vocabulary-space analysis for Transformer debugging."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "BertViz (multi-scale attention)",
      "plugin_id": "bertviz_attention",
      "task_input": ["classification", "generation"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "BertViz is a tool for visualizing self-attention in transformers at multiple scales. It provides views of individual heads, all heads, or neuron-level interactions.",
        "main_functionalities": [
          "Visualize attention patterns between tokens across layers and heads",
          "Provide multiple interactive views (head-level, model-level, neuron-level)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Multiscale Visualization of Attention in the Transformer Model",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1904.02679",
          "note": "Introduces BertViz tool and its views."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "exBERT (attention + context)",
      "task_input": ["classification"],
      "access_arch": {"access": ["white_box"], "arch": ["all"]},
      "target_scope": "local",
      "description": {
        "overview": "exBERT is a tool that explores transformer attention weights and contextual word representations. It finds similar contexts in a corpus to help interpret model predictions.",
        "main_functionalities": [
          "Visualize attention weights and token embeddings for a given input",
          "Retrieve and display similar input contexts from a dataset"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "exBERT: A Visual Analysis Tool to Explore Learned Representations",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1910.05276",
          "note": "Presents exBERT interactive tool."
        }
      ],
      "typical_cost": "medium"
    },
    {
      "name": "Sparse Autoencoders (SAELens + Neuronpedia)",
      "plugin_id": "sae_feature_explorer",
      "task_input": ["generation", "seq2seq", "general_NLP"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general"]},
      "target_scope": "both",
      "description": {
        "overview": "Sparse Autoencoders (SAEs) train a larger, sparse latent space to decompose transformer activations into features. Each sparse unit aims to capture a semantically meaningful concept【60†L23-L32】.",
        "main_functionalities": [
          "Train a sparse overcomplete dictionary on hidden activations",
          "Produce feature activations that reconstruct the original representation",
          "Enable analysis (visualization, ablation) of discovered features"
        ]
      },
      "strengths": [
        "Unsupervised discovery of latent feature directions",
        "Often yields more monosemantic features than raw neurons【60†L23-L32】",
        "Integrates with any model layer for interpretability"
      ],
      "limitations": [
        "Feature meanings must be interpreted (e.g., via top activations)",
        "No guarantee of true monosemantics; polysemantic features may remain",
        "Trade-off between reconstruction quality and sparsity"
      ],
      "research_applications": [
        {
          "used_in": "Towards Monosemanticity: Decomposing LMs with Dictionary Learning",
          "type": "paper",
          "year": 2023,
          "source": "ArXiv (Anthropic)",
          "url": "https://arxiv.org/abs/2306.09194",
          "note": "Introduces large-scale SAEs for discovering monosemantic features."
        },
        {
          "used_in": "Toy Models of Superposition",
          "type": "paper",
          "year": 2022,
          "source": "Transformer Circuits blog",
          "url": "https://transformer-circuits.pub/2022/toy_model/index.html",
          "note": "Explains the superposition hypothesis and role of sparse features."
        }
      ],
      "typical_cost": "high"
    },
    {
      "name": "Information Flow Routes",
      "plugin_id": "meta_transparency_graph",
      "task_input": ["generation"],
      "access_arch": {"access": ["white_box"], "arch": ["decoder", "encdec", "transformer_general"]},
      "target_scope": "both",
      "description": {
        "overview": "Information Flow Routes identifies a sparse computational subgraph that explains a model prediction. It computes attributions for intermediate components and prunes low-contribution edges to reveal key information pathways【60†L23-L32】.",
        "main_functionalities": [
          "Compute component contributions in one forward pass",
          "Build a directed graph of important residual-stream pathways",
          "Extract prediction-specific circuits without costly interventions"
        ]
      },
      "strengths": [
        "Scalable: single-pass attribution finds circuits efficiently【60†L23-L32】",
        "No need for custom corruption or extensive patching",
        "Highlights likely critical components for a prediction"
      ],
      "limitations": [
        "Correlational: not strictly causal",
        "Results depend on thresholding choices and greedy pruning",
        "Might miss alternate pathways or subtle effects"
      ],
      "research_applications": [
        {
          "used_in": "Information Flow Routes: Interpreting LMs at Scale",
          "type": "paper",
          "year": 2024,
          "source": "EMNLP",
          "url": "https://arxiv.org/abs/2404.07047",
          "note": "Introduces the Information Flow Routes technique."
        },
        {
          "used_in": "LM Transparency Tool (ACL 2024 Demo)",
          "type": "paper",
          "year": 2024,
          "source": "ACL Demo",
          "url": "https://aclanthology.org/2024.acl-demos.6.pdf",
          "note": "Presents a user interface leveraging Information Flow Routes."
        }
      ],
      "typical_cost": "medium"
    }, 
    {
      "name": "Integrated Gradients - Decoder Only (Inseq)",
      "plugin_id": "inseq_decoder_ig",
      "task_input": [
        "generation"
      ],
      "access_arch": {
        "access": [
          "mixed"
        ],
        "arch": [
          "decoder"
        ]
      },
      "target_scope": "local",
      "granularity": [
        "token"
      ],
      "user_goal_audience": [
        "research_debug",
        "general_tooling"
      ],
      "fidelity": "mixed",
      "format": [
        "API_only",
        "notebook_viz"
      ],
      "notes": "Builds on Captum for attribution in generation; target sequence or generated sequence; includes discretized IG and attention-based attribution."
    },
    {
      "name": "Integrated Gradients - EncoderDecoder (Inseq)",
      "plugin_id": "inseq_encdec_ig",
      "task_input": [
        "seq2seq"
      ],
      "access_arch": {
        "access": [
          "mixed"
        ],
        "arch": [
          "encdec"
        ]
      },
      "target_scope": "local",
      "granularity": [
        "token"
      ],
      "user_goal_audience": [
        "research_debug",
        "general_tooling"
      ],
      "fidelity": "mixed",
      "format": [
        "API_only",
        "notebook_viz"
      ],
      "notes": "Builds on Captum for attribution in generation; target sequence or generated sequence; includes discretized IG and attention-based attribution."
    },
    {
  "name": "PCA Embedding Visualization",
  "plugin_id": "embedding_pca_layers",
  "task_input": ["generation", "classification"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder", "encdec"]
  },
  "target_scope": "local",
  "description": {
    "overview": "Projects token representations (input embeddings and hidden states) into 2D using Principal Component Analysis (PCA) to visualize the geometry of transformer representations across layers. Users can choose a single fixed PCA basis (for cross-layer comparison) or per-layer bases (for layer-local structure).",
    "main_functionalities": [
      "Token-level PCA projections for any model layer",
      "Single-basis mode for cross-layer comparative visualization",
      "Per-layer PCA mode for exploring layer-specific representational structure",
      "Interactive layer selection and labeled scatter plots of tokens in PCA space"
    ]
  },
  "strengths": [
    "Provides intuitive visualization of representational geometry in high-dimensional spaces",
    "Reveals layerwise evolution of token representations",
    "Supports both fixed and per-layer PCA bases for flexible analysis"
  ],
  "limitations": [
    "PCA captures only linear structure and may miss non-linear geometry",
    "PCA of high-dimensional token embeddings misses a lot of important information and may destroy important semantic relationship",
    "Visualization quality depends on token count and sentence length"
  ],
  "research_applications": [
    {
      "used_in": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction",
      "type": "paper",
      "year": 2025,
      "source": "arXiv: Visualizing LLM Latent Space Geometry Through Dimensionality Reduction",
      "url": "https://arxiv.org/abs/2511.21594",
      "note": "Applies PCA (and UMAP) to analyze latent state geometries and layerwise evolution in transformer models."
    },
    {
      "used_in": "Examining Structure of Word Embeddings with PCA",
      "type": "paper",
      "year": 2019,
      "source": "International Conference of Text, Speech and Dialogue (TSD 2019)",
      "url": "https://arxiv.org/abs/1906.00114",
      "note": "Uses PCA to compare and interpret the geometric structure of word embeddings from different models."
    },
    {
      "used_in": "Learned Transformer Position Embeddings Have a Low-Dimensional Structure",
      "type": "paper",
      "year": 2024,
      "source": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP 2024)",
      "url": "https://aclanthology.org/2024.repl4nlp-1.17.pdf",
      "note": "Applies PCA to analyze the dimensionality and structure of positional and word embeddings in BERT models. They find that positional embeddings have a low-dimensional structure, using only about 10% of the available dimensions."
    }
    ] 
    }
  ]
}
