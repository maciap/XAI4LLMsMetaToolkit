{
  "methods": [
  {
  "name": "Integrated Gradients (Captum)",
  "plugin_id": "captum_ig_classifier",
  "task_input": ["classification", "generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["all"]
  },
  "target_scope": "local",
  "description": {
    "overview": "Integrated Gradients (IG) attributes a target class score to input tokens by integrating gradients along a path from a baseline input (e.g., PAD embedding) to the actual input. Compared to raw gradients, IG is typically more stable and less noisy for NLP token attribution.",
    "main_functionalities": [
      "Token-level attributions for a chosen class (predicted or user-specified)",
      "Baseline-based attributions (PAD embedding when available, otherwise zeros)",
      "Configurable integration steps (n_steps) to trade off stability vs speed"
    ]
  },
  "strengths": [
    "Usually more stable than plain gradients (Saliency) because it averages along an integration path",
    "Baseline-based framing is intuitive: ‘how much does the prediction change from baseline to input?’",
    "Good default for debugging text classifiers: tends to produce smoother token importances"
  ],
  "limitations": [
    "Baseline choice matters: different baselines can change attributions significantly",
    "More expensive than Saliency due to multiple forward/backward passes (n_steps)",
    "Still correlational: large attribution does not prove causal necessity without interventions"
  ],
  "research_applications": [
      {
      "used_in": "Axiomatic attribution for deep networks",
      "type": "paper",
      "year": 2017,
      "source": "Proceedings of the 34th International Conference on Machine Learning (ICML 2017)",
      "url": "https://dl.acm.org/doi/10.5555/3305890.3306024",
      "note": "Introduces the \"Integrated Gradients\" attribution method."
      }, 
       {
      "used_in": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
      "type": "paper",
      "year": 2023,
      "source": "NLP-OSS 2023",
      "url": "https://aclanthology.org/2023.nlposs-1.19/",
      "note": "Discusses features added to the Captum library (Captum v0.7) for language models."

      }
      ]
    },
    {
      "name": "Layer Integrated Gradients (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Integrated-Gradients-style attribution applied to internal layers to attribute importance to intermediate representations rather than inputs.",
        "main_functionalities": [
          "Compute attributions for intermediate layer inputs/outputs",
          "Enable layer-wise analysis beyond raw token saliency"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
  "name": "Saliency (Captum)",
  "plugin_id": "captum_saliency_classifier",
  "task_input": ["classification", "generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["transformer_general", "encoder", "NA"]
  },
  "target_scope": "local",
  "granularity": ["token", "layer", "NA"],
  "user_goal_audience": ["research_debug", "general_tooling"],
  "fidelity": "low",
  "format": ["visual_UI", "API_only", "notebook_viz"],
  "description": {
    "overview": "Saliency computes token importance using the gradient of the target class score with respect to the input embeddings. It’s fast and simple, but often noisier than baseline-based methods like Integrated Gradients.",
    "main_functionalities": [
      "Fast token-level sensitivity scores for a chosen class",
      "No baseline required (single backward pass)",
      "Useful for quick inspection / debugging loops"
    ]
  },
  "strengths": [
    "Very fast (typically a single forward+backward pass)",
    "Good for rapid debugging: ‘which tokens is the logit most sensitive to?’",
    "Simple to explain and easy to implement"
  ],
  "limitations": [
    "Often noisy / unstable; may highlight spurious sensitivities",
    "Can suffer from gradient saturation and discontinuities",
    "Sensitivity ≠ importance: high gradient may not correspond to causal influence"
  ],
  "research_applications": [
      {
      "used_in": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "type": "paper",
      "year": 2014,
      "source": "Workshop Track Proceedings of the 2nd International Conference on Learning Representations (ICLR 2014)",
      "url": "https://arxiv.org/pdf/1312.6034",
      "note": "Introduces the \"Saliency\" attribution method."
      }, 
       {
      "used_in": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
      "type": "paper",
      "year": 2023,
      "source": "NLP-OSS 2023",
      "url": "https://aclanthology.org/2023.nlposs-1.19/",
      "note": "Discusses features added to the Captum library (Captum v0.7) for language models."

      }
      ]
    }   ,
    {
      "name": "Input×Gradient (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Attribution method that multiplies the input features (often embeddings) by the gradient signal, yielding signed token contributions in embedding space.",
        "main_functionalities": [
          "Compute signed contributions by combining gradients with input magnitudes"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
  "name": "DeepLift (Captum)",
  "plugin_id": "captum_deeplift_classifier",
  "task_input": ["classification", "generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["transformer_general", "encoder", "NA"]
  },
  "target_scope": "local",
  "description": {
    "overview": "DeepLift attributes a target class score by comparing activations to a baseline and propagating contribution differences backward. In practice it can be sharper than plain gradients when gradients saturate, while remaining cheaper than high-step IG.",
    "main_functionalities": [
      "Baseline-based token attributions for a chosen class",
      "Often better behaved than plain gradients under saturation",
      "Useful alternative when IG is too slow or baseline sensitivity is acceptable"
    ]
  },
  "strengths": [
    "Baseline-based like IG, but often cheaper than high-step IG",
    "Can handle some saturation cases better than raw gradients",
    "Often yields clearer token attribution patterns than Saliency"
  ],
  "limitations": [
    "Baseline choice matters (similar to IG)",
    "Can behave unexpectedly depending on model components and non-linearities",
    "Still not causal: attributions can be overridden/cancelled by downstream computations"
  ],
  "research_applications": [
      {
      "used_in": "Learning Important Features Through Propagating Activation Differences",
      "type": "paper",
      "year": 2017,
      "source": "Proceedings of the 34th International Conference on Machine Learning (ICML 2017)",
      "url": "https://proceedings.mlr.press/v70/shrikumar17a.html",
      "note": "Introduces the \"DeepLift\" attribution method."
      }, 
       {
      "used_in": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
      "type": "paper",
      "year": 2023,
      "source": "NLP-OSS 2023",
      "url": "https://aclanthology.org/2023.nlposs-1.19/",
      "note": "Discusses features added to the Captum library (Captum v0.7) for language models."

      }
      ]
    },
    {
      "name": "GradientSHAP (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Gradient-based Shapley-style attribution variant (as implemented in Captum) combining ideas from SHAP baselines and gradient integration/approximation.",
        "main_functionalities": [
          "Compute attributions using gradients with randomized baselines / sampling (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html",
          "note": "Introduces SHAP as a unified framework; gradient-based variants appear in later tool implementations."
        }
      ]
    },
    {
      "name": "Feature Ablation (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation-based attribution method that measures output change when features/tokens are replaced by a baseline value.",
        "main_functionalities": [
          "Estimate importance by ablation (feature removal/replacement) and observing output differences"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Occlusion (Captum)",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation method that occludes parts of the input (e.g., tokens/spans) and measures the effect on the model output.",
        "main_functionalities": [
          "Compute importance by systematically occluding input segments and measuring output change"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Visualizing and Understanding Convolutional Networks",
          "type": "paper",
          "year": 2014,
          "source": "ECCV (Springer)",
          "url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53",
          "note": "Popularizes occlusion-style sensitivity analyses for interpreting neural models."
        }
      ]
    },
    {
      "name": "Kernel SHAP (InterpretML: ShapKernel)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Model-agnostic Shapley-value estimation using a Kernel SHAP-style procedure exposed in InterpretML as ShapKernel.",
        "main_functionalities": [
          "Compute local feature attributions for black-box predictors",
          "Aggregate attributions over a dataset for global summaries (workflow-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "A Unified Approach to Interpreting Model Predictions",
          "type": "paper",
          "year": 2017,
          "source": "NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html",
          "note": "Introduces SHAP."
        }
      ]
    },
    {
      "name": "LIME (InterpretML: LimeTabular)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Local surrogate explanation method that fits an interpretable model around a specific instance using perturbed samples (InterpretML exposes a tabular variant as LimeTabular).",
        "main_functionalities": [
          "Generate local explanations by learning a simple surrogate model around an instance"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
          "type": "paper",
          "year": 2016,
          "source": "KDD",
          "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
          "note": "Introduces LIME."
        }
      ]
    },
    {
      "name": "Partial Dependence Plot (InterpretML: PartialDependence)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "global",
      "description": {
        "overview": "Global explanation that estimates the marginal effect of one (or more) features on the model prediction.",
        "main_functionalities": [
          "Compute marginal feature–prediction relationships over a dataset"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Morris Sensitivity Analysis (InterpretML: MorrisSensitivity)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "global",
      "description": {
        "overview": "Global sensitivity-analysis method that estimates how input features influence model outputs by sampling along trajectories in the input space.",
        "main_functionalities": [
          "Quantify feature influence via Morris-style sensitivity indices (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "SageMaker Clarify SHAP feature attributions",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Shapley-value-based feature attributions produced by SageMaker Clarify, documented as available for both specific predictions (local) and global model understanding; includes NLP explainability configuration for text features.",
        "main_functionalities": [
          "Compute Shapley-based feature attributions for predictions",
          "Provide dataset-level aggregation for global importance (as documented)",
          "Support NLP explainability with text-specific configuration"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud",
          "type": "paper",
          "year": 2021,
          "source": "Amazon Science (PDF)",
          "url": "https://assets.amazon.science/45/76/30bab4f14ccab96cfe8067ed2b4a/amazon-sagemaker-clarify-machine-learning-bias-detection-and-explainability-in-the-cloud.pdf",
          "note": "Describes Clarify’s SHAP-based explainability at scale."
        }
      ]
    },
    {
      "name": "SageMaker Clarify Partial Dependence Plots",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "global",
      "description": {
        "overview": "Partial Dependence Plots (PDP) generation in SageMaker Clarify to visualize marginal feature effects on model predictions.",
        "main_functionalities": [
          "Generate PDPs for selected features",
          "Support model-level explainability reports including PDP artifacts (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret Gradient attribution",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token-level feature attribution using input gradients (optionally multiplied by token embeddings) for transformer text classifiers.",
        "main_functionalities": [
          "Compute token attribution scores via gradients",
          "Return per-token signed relevance for a target class"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret Integrated Gradients attribution",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token-level Integrated Gradients attribution (optionally × token embeddings) for transformer text classifiers.",
        "main_functionalities": [
          "Compute IG-style token attributions relative to a baseline"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret SHAP (Partition SHAP approximation)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Token-level SHAP-style attribution in ferret, described as using a Partition SHAP approximation of Shapley values.",
        "main_functionalities": [
          "Compute SHAP-like token relevance scores for a prediction",
          "Support aggregation workflows for benchmarking and comparison"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "ferret LIME token attribution",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token-level LIME explanations for transformer text classifiers via a local surrogate model.",
        "main_functionalities": [
          "Estimate token relevance using local perturbations and a surrogate model fit"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "transformers-interpret Integrated Gradients",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Token attribution for Hugging Face transformers built around Integrated Gradients, returning signed per-token attributions for a chosen class/output.",
        "main_functionalities": [
          "Produce token-level attributions for transformer classifiers",
          "Provide notebook-friendly visualization outputs"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "transformers-interpret Layer Integrated Gradients",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Layer-level IG variant used in transformers-interpret for transformer models, described as a core attribution method alongside IG.",
        "main_functionalities": [
          "Compute attributions with respect to internal layers (implementation-specific)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Interpret-Text ClassicalTextExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Explainer for classical text pipelines using bag-of-words encoding and interpretable model parameters (e.g., logistic regression weights or tree feature importances) to surface word importances.",
        "main_functionalities": [
          "Handle text preprocessing/encoding in its default pipeline (bag-of-words)",
          "Train or wrap supported linear/tree models and expose word importances as explanations",
          "Provide local word importances and support label-wise/global inspection via dashboard workflows"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Interpret-Text UnifiedInformationExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Information-based measure used to explain how intermediate layers of deep NLP models encode information of input words; implemented for BERT in Interpret-Text.",
        "main_functionalities": [
          "Compute layer-wise, word-level quantitative explanations across BERT transformer layers (as implemented)",
          "Support inspection across transformer, pooler, and classification layers (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [
        "Implementation described as BERT-only in Interpret-Text."
      ],
      "research_applications": [
        {
          "used_in": "Towards a Deep and Unified Understanding of Deep Neural Models in NLP",
          "type": "paper",
          "year": 2019,
          "source": "PMLR (ICML 2019)",
          "url": "https://proceedings.mlr.press/v97/guan19a.html",
          "note": "Introduces the unified information-based measure referenced by the Interpret-Text explainer."
        }
      ]
    },
    {
      "name": "Interpret-Text IntrospectiveRationaleExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Generator–predictor selective rationalization framework that produces rationales (and anti-rationales) by incorporating predicted outcomes into the selection process.",
        "main_functionalities": [
          "Generate a subset of input tokens/spans intended to be sufficient for the classification outcome (rationales)",
          "Optionally identify complement/anti-rationales (tokens not useful for prediction), as described in the toolkit README"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1910.13294",
          "note": "Introduces the introspective rationalization approach referenced by Interpret-Text."
        }
      ]
    },
    {
      "name": "Interpret-Text Likelihood Explainer",
      "task_input": ["generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation-based local explanation for generative text models using conditional log-likelihood (log-probability) measurements to attribute importance to text segments.",
        "main_functionalities": [
          "Create perturbed versions of input text",
          "Measure changes in conditional log likelihood under perturbations to assign local importances",
          "Require access to model log probabilities (as described)"
        ]
      },
      "strengths": [
        "Model-agnostic given access to log probabilities (as described)."
      ],
      "limitations": [
        "Requires models/interfaces that provide log probabilities."
      ],
      "research_applications": []
    },
    {
      "name": "Interpret-Text Sentence Embedder Explainer",
      "task_input": ["generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Perturbation-based local explanation that does not require log probabilities; uses sentence embeddings to compare perturbed outputs to the original output and assign importance based on embedding proximity.",
        "main_functionalities": [
          "Generate perturbed versions of the input",
          "Compare outputs via embedding similarity/proximity to the original output",
          "Assign importance to segments based on impact on embedding proximity (as described)"
        ]
      },
      "strengths": [
        "Does not require log probabilities (as described), enabling use with API-only models that do not expose log-likelihood."
      ],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Interpret-Text Hierarchical Text Explainer",
      "task_input": ["generation"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Hierarchical perturbation explainer for generative models that analyzes text at multiple granularities (sentence level down to n-gram partitions) to identify influential components, optionally using either likelihood-based or embedding-based local explanation backends.",
        "main_functionalities": [
          "Apply perturbations across multiple text scales (sentence to n-gram partitions)",
          "Measure output changes to assign importance at varying levels of detail",
          "Support combination with likelihood-based or sentence-embedding-based local explanation backends (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "PnPXAI Attention Rollout",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Attention-based attribution method that aggregates attention information across layers/heads to produce token-level relevance estimates (as exposed by PnPXAI).",
        "main_functionalities": [
          "Compute attention-based relevance maps via attention aggregation/rollout (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "PnPXAI KernelShap",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "all",
      "description": {
        "overview": "Model-agnostic Kernel SHAP explainer exposed by PnPXAI.",
        "main_functionalities": [
          "Compute local Shapley-style feature attributions for a predictor"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models",
          "type": "paper",
          "year": 2025,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2505.10515",
          "note": "Describes PnPXAI and evaluates multiple explainers including KernelSHAP."
        }
      ]
    },
    {
      "name": "DALEX BreakDown",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Prediction-level variable attribution method that decomposes a single prediction into additive contributions along an ordered path (BreakDown).",
        "main_functionalities": [
          "Compute per-feature contributions for a single instance",
          "Support interaction-aware variants (implementation-dependent)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "DALEX Shap (random-path Shapley values)",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Prediction-level Shapley-value attribution in DALEX computed by averaging attributions across multiple random feature orderings (as documented).",
        "main_functionalities": [
          "Compute Shapley-style attributions for a single instance using sampled paths"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "FAT Forensics CounterfactualExplainer",
      "task_input": ["classification"],
      "access_arch": { "access": ["black_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Counterfactual explanation generator for black-box classifier predictions (as exposed in FAT Forensics).",
        "main_functionalities": [
          "Generate counterfactual instances intended to change the model prediction",
          "Provide utilities to textualize/describe counterfactuals (as documented)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and Transparency",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1909.05167",
          "note": "Describes the FAT Forensics toolbox and its scope including transparency components."
        }
      ]
    },
    {
      "name": "Activation Patching (TransformerLens)",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Causal intervention technique that patches internal activations from a ‘clean’ run into a ‘corrupted’ run and measures the effect on model outputs; TransformerLens provides utilities implementing activation patching across activation types.",
        "main_functionalities": [
          "Patch chosen activations between runs and measure downstream output changes",
          "Localize internal components important for a specific behavior/prediction"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    },
    {
      "name": "Path Patching",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Refined patching variant where each patch is constrained to affect only a single target component, described as patching the ‘path’ between two components.",
        "main_functionalities": [
          "Test whether effects propagate directly between two components versus via mediation",
          "Conduct more targeted causal tracing than unconstrained activation patching"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "How to use and interpret activation patching",
          "type": "paper",
          "year": 2024,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2404.15255",
          "note": "Defines and discusses path patching as a constrained form of patching."
        }
      ]
    },
    {
      "name": "Attribution Patching",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Gradient-based approximation technique described as enabling many activation-patching measurements efficiently using a small number of forward/backward passes.",
        "main_functionalities": [
          "Approximate patching effects with gradient-based computations (as described)",
          "Scale patching-style analysis to many components/patches more efficiently"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Attribution Patching: Activation Patching At Industrial Scale",
          "type": "blog",
          "year": 2026,
          "source": "neelnanda.io",
          "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching",
          "note": "Introduces attribution patching as a gradient-based approximation to activation patching."
        }
      ]
    },
    {
      "name": "VISIT semantic information flow graph",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Interactive visualization method/tool that represents a forward pass as a flow graph (nodes as neurons/hidden states; edges as interactions) grounded in vocabulary-space projections to study information flow in GPT-style models.",
        "main_functionalities": [
          "Construct forward-pass flow graphs for inspection",
          "Support analysis of attention heads and ‘memory values’ via vocabulary projections (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers",
          "type": "paper",
          "year": 2023,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2305.13417",
          "note": "Introduces VISIT and its interactive flow-graph approach."
        }
      ]
    },
    {
      "name": "Vocabulary-space FFN update decomposition",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "local",
      "description": {
        "overview": "Mechanistic interpretability method that views token representations as distributions over vocabulary and interprets FFN outputs as additive updates in vocabulary space, decomposable into sub-updates linked to parameter vectors promoting interpretable concepts.",
        "main_functionalities": [
          "Project FFN-induced representation changes into vocabulary space",
          "Decompose updates into sub-components associated with FFN parameter vectors (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2203.14680",
          "note": "Introduces the vocabulary-space FFN update interpretation and decomposition."
        },
        {
          "used_in": "LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models",
          "type": "paper",
          "year": 2022,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/2204.12130",
          "note": "Uses this vocabulary-space method as the backbone for debugging and interventions."
        }
      ]
    },
    {
      "name": "BertViz multi-scale attention visualization",
      "task_input": ["classification", "generation"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Interactive visualization method/tool for inspecting multi-head self-attention at multiple granularities (head-, model-, and neuron-level views, as described).",
        "main_functionalities": [
          "Visualize attention patterns across layers and heads",
          "Provide multiple views to inspect attention structure and related signals"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "Visualizing Attention in Transformer-Based Language Representation Models",
          "type": "paper",
          "year": 2019,
          "source": "arXiv",
          "url": "https://arxiv.org/abs/1904.02679",
          "note": "Describes BertViz and its multi-scale attention visualizations."
        }
      ]
    },
    {
      "name": "exBERT attention + contextual representation exploration",
      "task_input": ["classification"],
      "access_arch": { "access": ["white_box"], "arch": ["all"] },
      "target_scope": "local",
      "description": {
        "overview": "Interactive analysis method/tool that explores attention weights and contextual representations and matches input contexts to similar contexts in an annotated dataset to aid interpretation.",
        "main_functionalities": [
          "Visualize attention and contextual embeddings for transformer inputs",
          "Retrieve/match similar contexts from a corpus and aggregate annotations (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": [
        {
          "used_in": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models",
          "type": "paper",
          "year": 2019,
          "source": "ar5iv/arXiv HTML",
          "url": "https://ar5iv.labs.arxiv.org/html/1910.05276",
          "note": "Introduces exBERT and its context-matching interpretation mechanism."
        }
      ]
    },
    {
      "name": "MechaMap neuron category scanning",
      "task_input": ["generation"],
      "access_arch": { "access": ["white_box"], "arch": ["decoder"] },
      "target_scope": "global",
      "description": {
        "overview": "Single-pass scanning workflow intended to surface neurons that respond strongly to predefined semantic categories by running a ‘master text’ and measuring activations at MLP outputs (as described in the project README).",
        "main_functionalities": [
          "Run a configured ‘master text’ and compute neuron activations by category",
          "Export ranked neurons / top-activating tokens for downstream inspection (as described)"
        ]
      },
      "strengths": [],
      "limitations": [],
      "research_applications": []
    }, 
    {
      "name": "Integrated Gradients - Decoder Only (Inseq)",
      "plugin_id": "inseq_decoder_ig",
      "task_input": [
        "generation"
      ],
      "access_arch": {
        "access": [
          "white-box"
        ],
        "arch": [
          "decoder"
        ]
      },
      "target_scope": "local",
      "description": {
        "overview": "Integrated Gradient is a popular gradient-based attribution method that assigns importance scores to input tokens by integrating gradients along a path from a baseline input to the actual input. This implementation focuses on decoder-only models and supports attribution for both target sequences and generated sequences",
        "main_functionalities": [
        "Assign attribution scores to pairs of input and output tokens"
      ]
      },
      "research_applications": [
    {
      "used_in": "Axiomatic attribution for deep networks",
      "type": "paper",
      "year": 2017,
      "source": "Proceedings of the 34th International Conference on Machine Learning (ICML 2017)",
      "url": "https://dl.acm.org/doi/10.5555/3305890.3306024",
      "note": "Introduces the \"Integrated Gradients\" attribution method."
    },   
    {
      "used_in": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
      "type": "demo paper",
      "year": 2023,
      "source": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2023)",
      "url": "https://aclanthology.org/2023.acl-demo.40.pdf",
      "note": "Presents the Inseq toolkit for interpreting sequence generation via large language models."
    }
    ], 
    "strengths": [
    "Enables simple and theoretically-grounded token attribution in the challenging sequence generation scenario"
  ],
  "limitations": [
    "The choice of the baseline is difficult to define for text input", 
    "The straight line used for intergrating gradients is not meaningful for (discrete) text input"
  ]
  },
      {
      "name": "Integrated Gradients - EncoderDecoder (Inseq)",
      "plugin_id": "inseq_encdec_ig",
      "task_input": [
        "generation"
      ],
      "access_arch": {
        "access": [
          "generation"
        ],
        "arch": [
          "encdec"
        ]
      },
      "target_scope": "local",
      "description": {
        "overview": "Integrated Gradient is a popular gradient-based attribution method that assigns importance scores to input tokens by integrating gradients along a path from a baseline input to the actual input. This implementation focuses on encoder-decoder models and supports attribution for both target sequences and generated sequences",
        "main_functionalities": [
        "Assign attribution scores to pairs of input and output tokens"
      ]
      },
      "research_applications": [
    {
      "used_in": "Axiomatic attribution for deep networks",
      "type": "paper",
      "year": 2017,
      "source": "Proceedings of the 34th International Conference on Machine Learning (ICML 2017)",
      "url": "https://dl.acm.org/doi/10.5555/3305890.3306024",
      "note": "Introduces the \"Information flow routes\" technique to automatically and efficientlyextract sparse computational circuits in large language models."
    },   
    {
      "used_in": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
      "type": "demo paper",
      "year": 2023,
      "source": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2023)",
      "url": "https://aclanthology.org/2023.acl-demo.40.pdf",
      "note": "Presents the Inseq toolkit for interpreting sequence generation via large language models."
    }
    ], 
    "strengths": [
    "Enables simple and theoretically-grounded token attribution the challenging sequence generation scenario"
   ],
   "limitations": [
    "The choice of the baseline is difficult to define for text input", 
    "The straight line used for intergrating gradients is not meaningful for (discrete) text input"
    ]
    }, 
    {
  "name": "Information Flow Routes (LLM Transparency Tool)",
  "plugin_id": "meta_transparency_graph",
  "task_input": ["generation"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder", "encdec", "transformer_general"]
  },
  "target_scope": "both",
  "description": {
    "overview": "Information flow routes is a technique rooted in mechanistic interpretability research that, given a graph were nodes represent residual stream of tokens and edges computations, automatically identifies sparse computational subgraphs inside transformer models for specific predictions. Instead of using  expensive activation-patching interventions, the method computes edge attributions based on proximity to the sum of all edges pointing to a node  (residual stream).  By recursively pruning out low-contribution edges in a top-down manner, the information flow routes technique constructs a sparse directed graph representing the computational routes responsible for a prediction.",
    "main_functionalities": [
      "Efficiently compute attribution to intermediate transformer components using a single forward pass",
      "Automatically build prediction-specific information-flow graphs highlighting the most important components for the prediction",
      "Enable large-scale authomatic circuit discovery"
    ]
  },
  "research_applications": [
    {
      "used_in": "Information flow routes: Automatically interpreting language models at scale",
      "type": "paper",
      "year": 2024,
      "source": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024)",
      "url": "https://arxiv.org/abs/2404.07047",
      "note": "Introduces the \"Information flow routes\" technique to automatically and efficientlyextract sparse computational circuits in large language models."
    },
    {
      "used_in": "LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models",
      "type": "demo paper",
      "year": 2024,
      "source": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)",
      "url": "https://aclanthology.org/2024.acl-demos.6.pdf",
      "note": "Presents the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. This toolkit is based on the \"Information flow routes\" technique and provides a user-friendly interface for exploring the import computational routes responsible for specific predictions in language models."
    }
  ],
  "strengths": [
    "Enables simple and efficient attribution of model components", 
    "Enables single-pass, scalable extraction of prediction-specific computational subgraphs without expensive patching",
    "Unlike alternative patching-based circuit discovery methods, it does not require handcrafted corruption templates"
  ],
  "limitations": [
    "The attribution of model components adopted by the method is correlational. It approximates causal influence and is not strictly interventional",
    "Thresholding choices affect sparsity and graph structure",
    "Greedy choices in extracting the subgraph given the attribution may lead to poor results"
  ]
}, 
{
  "name": "Sparse Autoencoders (SAELens + Neuronpedia)",
  "plugin_id": "sae_feature_explorer",
  "task_input": ["generation", "seq2seq", "general_NLP"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder", "encdec", "transformer_general"]
  },
  "target_scope": "both",
  "description": {
    "overview": "Sparse Autoencoders (SAEs) are used to discover interpretable features inside transformer latent representations. According to the superposition hypothesis, neural networks can represent more features than they have dimensions by encoding them sparsely, at the cost of interpretability. SAEs attempt to reverse this superposition by learning a new sparse basis of higher dimension over neuron activations, where each latent dimension ideally corresponds to a more monosemantic concept. ",
    "main_functionalities": [
    "Learn a sparse, overcomplete dictionary of latent features that reconstruct neural activations",
    "Enable inspection of feature activations at specific token positions or contexts",
    "Support downstream interpretability workflows such as feature visualization, steering, ablation, and attribution"
  ]
  },
  "research_applications": [
    {
      "used_in": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning",
      "type": "paper",
      "year": 2023,
      "source": "Anthropic",
      "url": "https://arxiv.org/abs/2306.09194",
      "note": "Introduces large-scale sparse autoencoders for discovering monosemantic features in transformer activations."
    },
    {
      "used_in": "Toy Models of Superposition",
      "type": "paper",
      "year": 2022,
      "source": "Transformer Circuits",
      "url": "https://transformer-circuits.pub/2022/toy_model/index.html",
      "note": "Explains the superposition hypothesis and why sparse feature discovery is needed."
    },
    {
      "used_in": "Scaling and Evaluating Sparse Autoencoders",
      "type": "paper",
      "year": 2024,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2406.04093",
      "note": "Introduces top-k sparse autoencoders and analyzes scaling behavior and dead latents."
    },
    {
      "used_in": "Improving Dictionary Learning with Gated Sparse Autoencoders",
      "type": "paper",
      "year": 2024,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2404.16014",
      "note": "Proposes gated autoencoders that separate feature selection from feature magnitude to mitigate shrinkage bias."
    },
    {
      "used_in": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders",
      "type": "paper",
      "year": 2024,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2407.14435",
      "note": "Introduces JumpReLU SAEs trained directly on an L0 objective using straight-through estimation."
    },
    {
      "used_in": "SAELens",
      "type": "documentation",
      "year": 2024,
      "source": "decoderesearch.github.io",
      "url": "https://decoderesearch.github.io/SAELens/",
      "note": "Library for loading and analyzing pretrained sparse autoencoders on TransformerLens models."
    }
  ],
  "strengths": [
    "Unsupervised feature discovery: does not require predefined concepts or labels",
    "Often produces more monosemantic directions than raw neurons",
    "Can be integreated in any model and in any layer",
    "Enables advanced interventions (steering, ablation, attribution) using the discovered features"
  ],
  "limitations": [
    "Feature meaning is not automatic: interpretation typically requires inspecting top-activating examples",
    "There is no guarantee that features will be monosemantic. Features can still be polysemantic, especially in small or weakly sparse SAEs",
    "Reconstruction–sparsity trade-off: stronger sparsity improves interpretability but may hurt reconstruction quality",
    "Global analysis of a model requires large-scale activation datasets"
  ]
}, 
{
  "name": "Anchors",
  "plugin_id": "alibi_anchors_text",
  "task_input": ["classification"],
  "access_arch": {
    "access": ["black_box"],
    "arch": ["transformer_general"]
  },
  "description": {
    "overview": "AnchorText is a local, model-agnostic explanation method that identifies a small set of words (an anchor) in a text such that, if those words are present, the model’s prediction is highly likely to remain the same. Rather than assigning numeric importance scores like SHAP, it produces IF-THEN style rules — showing combinations of features that reliably “anchor” a prediction under perturbations of the input.",
    "main_functionalities": [
      "Retrieves a set of words (‘anchor’) in text that ‘locks in’ the model’s prediction locally",
      "Provides quantitative metrics — precision (reliability) and coverage (scope) — for the retrieved set of words",
      "Works with any classifier without requiring access to model internals",
      "Returns example texts where the anchor holds and where it fails for qualitative inspection"
    ]
  },
  "research_applications": [
    {
      "used_in": "Anchors: High-Precision Model-Agnostic Explanations",
      "type": "paper",
      "year": 2018,
      "source": "AAAI",
      "url": "https://homes.cs.washington.edu/~marcotcr/aaai18.pdf",
      "note": "Original paper introducing the Anchors algorithm for model-agnostic explanations using high-precision local rules. Shows user studies where anchors allow users to predict model behavior with less effort than other explainers. :contentReference[oaicite:0]{index=0}"
    },
    {
      "used_in": "A Sea of Words: An In-Depth Analysis of Anchors for Text Data",
      "type": "paper",
      "year": 2022,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2205.13789",
      "note": "Analyzes the theoretical properties of Anchors specifically for text classifiers, showing how small sets of words can explain model decisions under different conditions. :contentReference[oaicite:1]{index=1}"
    },
    {
      "used_in": "Understanding Post-hoc Explainers: The Case of Anchors",
      "type": "paper",
      "year": 2023,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2303.08806",
      "note": "Provides theoretical and empirical analysis of the Anchors explainer and discusses strengths and limitations of the approach. :contentReference[oaicite:2]{index=2}"
    },
    {
      "used_in": "Alibi Explain: Algorithms for Explaining ML Models",
      "type": "paper",
      "year": 2021,
      "source": "JMLR Library",
      "url": "https://jmlr.csail.mit.edu/papers/volume22/21-0017/21-0017.pdf",
      "note": "Describes the Alibi library that implements AnchorText among other explainability methods for text, tabular, and image data. :contentReference[oaicite:3]{index=3}"
    }
  ],
  "strengths": [
    "Produces intuitive IF-THEN style explanations that are easy for humans to understand and communicate",
    "Model-agnostic: works with black-box classifiers without needing gradients or internal architecture specifics",
    "Quantifies reliability (precision) and breadth (coverage) of the explanation rule",
    "Returns concrete example texts where the anchor holds and where it fails, aiding debugging and trust assessment"
  ],
  "limitations": [
    "Focused on local explanations: does not provide a global summary of model behavior",
    "Can be computationally heavy for long texts or strict precision thresholds due to sampling perturbations",
      "Explanation quality depends on the perturbation strategy used to generate local text variants; heuristic replacements (e.g., the use of the UNK token) can produce unnatural or context-less sentences",   
    "Explainer may return trivial anchors or none if the model behavior is diffuse (words are weakly predictive)"
  ]
},
{
  "name": "Direct Logit Attribution",
  "plugin_id": "direct_logit_attribution",
  "task_input": ["generation", "seq2seq", "general_NLP", "NA"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder", "encdec", "transformer_general", "NA"]
  },
  "description": {
    "overview": "A diagnostic technique that attributes a chosen target token’s logit to internal transformer components by projecting each component’s output vector onto the target token’s unembedding direction (LM head). Positive contributions push the model toward the target token; negative contributions push it away.",
    "main_functionalities": [
      "Compute signed contributions of transformer components (e.g., per-layer attention output, per-layer MLP output) to a chosen target token logit at a chosen position",
      "Rank components by absolute contribution for quick identification of which layers/components most strongly support or oppose a target prediction",
      "Support both automatic target selection (model’s predicted next token) and manual target token selection (single-token string / token id)",
      "Return JSON-friendly, human-readable outputs for UI tables/plots (component name, type, layer index, signed contribution, absolute share)"
    ]
  },
  "research_applications": [
    {
      "used_in": "Attribution Patching: Activation Patching At Industrial Scale",
      "type": "blog",
      "year": 2023,
      "source": "neelnanda.io",
      "url": "https://www.neelnanda.io/mechanistic-interpretability/attribution-patching",
      "note": "Discusses direct logit attribution as a simple instance of (direct) path patching to logits: measuring the direct contribution of component outputs to logits."
    },
    {
      "used_in": "TransformerLens documentation (ActivationCache.decompose_resid)",
      "type": "documentation",
      "year": 2023,
      "source": "transformerlensorg.github.io",
      "url": "https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.ActivationCache.html",
      "note": "Provides tooling for decomposing the residual stream into component outputs, explicitly motivated by use-cases like direct logit attribution."
    },
    {
      "used_in": "A Mathematical Framework for Transformer Circuits",
      "type": "article",
      "year": 2021,
      "source": "Transformer Circuits Thread",
      "url": "https://transformer-circuits.pub/2021/framework/index.html",
      "note": "Popularizes the residual-stream-as-sum-of-components view, which motivates projecting component outputs into logit space to interpret which parts write evidence for a prediction."
    },
    {
      "used_in": "An Adversarial Example for Direct Logit Attribution: Memory Management in GELU-4L",
      "type": "paper",
      "year": 2024,
      "source": "arXiv",
      "url": "https://arxiv.org/html/2310.07325v4",
      "note": "Shows concrete cases where DLA can be misleading under erasure/cleanup (memory management), highlighting important failure modes."
    }
  ],
  "strengths": [
    "Fast and simple: requires only one forward pass plus linear projections of component outputs into the unembedding direction",
    "Highly interpretable sign: positive contributions provide direct evidence for the target token; negative contributions provide direct counter-evidence",
    "Useful for locating the ‘end of a circuit’: quickly identifies which layers/components most directly write toward a specific token prediction",
    "Works well as a companion to causal methods: DLA is a good first diagnostic before doing more expensive interventions (activation patching / path patching)"
  ],
  "limitations": [
    "Not causal: a large positive contribution does not imply the component is necessary for the prediction (downstream components may override/cancel it)",
    "Ignores softmax competition (attention-weight coupling) and other nonlinearities: the component output is treated as an additive vector even though it was produced by nonlinear computation",
    "LayerNorm and scaling can matter: whether you project pre/post normalization affects interpretability; different architectures apply norms in different places",
    "Can be misleading under ‘cleanup/erasure’ behavior where later components actively remove earlier-written directions from the residual stream"
  ]
}, 
   {
  "name": "Logit Lens",
  "plugin_id": "logit_lens",
  "task_input": ["generation", "seq2seq", "general_NLP", "NA"],
  "access_arch": {
    "access": ["white_box"],
    "arch": ["decoder", "encdec", "transformer_general", "NA"]
  },
  "description": {
    "overview": "A diagnostic technique that projects intermediate hidden states through the model’s unembedding (LM head) to see which vocabulary tokens each layer is ‘leaning toward’ predicting at a chosen position.",
    "main_functionalities": [
      "Layer-by-layer top-k token predictions for a chosen token position",
      "Track a token’s score/probability across layers (e.g., final-layer top token)",
      "Optional normalization choices to make intermediate-layer comparisons more meaningful (e.g., handling final LayerNorm)"
    ]
  },
  "research_applications": [
    {
      "used_in": "Interpreting GPT: the logit lens (original write-up / introduction)",
      "type": "blog",
      "year": 2020,
      "source": "LessWrong",
      "url": "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens",
      "note": "Introduces the basic idea: unembed intermediate residual stream states to inspect evolving token predictions."
    },
    {
      "used_in": "A Mathematical Framework for Transformer Circuits",
      "type": "article",
      "year": 2021,
      "source": "Transformer Circuits Thread",
      "url": "https://transformer-circuits.pub/2021/framework/index.html",
      "note": "Popularizes the residual-stream + unembedding viewpoint and the idea of interpreting intermediate states via logits."
    },
    {
      "used_in": "Visualizing and Interpreting the Semantic Information Flow of Transformers (Findings of EMNLP 2023)",
      "type": "paper",
      "year": 2023,
      "source": "ACL Anthology (PDF)",
      "url": "https://aclanthology.org/2023.findings-emnlp.939.pdf",
      "note": "Applies a logit-lens-style projection before/after LayerNorm to study information flow and component effects."
    },
    {
      "used_in": "What Language Do Non-English-Centric Large Language Models Think In? (Findings of ACL 2025)",
      "type": "paper",
      "year": 2025,
      "source": "ACL Anthology (PDF)",
      "url": "https://aclanthology.org/2025.findings-acl.1350.pdf",
      "note": "Uses logit lens as a measurement tool to detect latent language signals across layers."
    }
  ],
  "strengths": [
    "Fast and simple: a single forward pass can give layer-wise token tendencies",
    "Great for mechanistic debugging: shows *when* a representation becomes linearly decodable into the final vocabulary space",
    "Works naturally with decoder LMs (and often encoder-decoder) without needing perturbation sampling"
  ],
  "limitations": [
    "Not causal: shows what is decodable via the unembedding, not what is necessary for the final prediction",
    "LayerNorm / scaling matters: intermediate states may not be comparable to final-layer states without careful normalization",
    "Top-k tokens can be misleading when probability mass is diffuse; consider adding entropy/margin diagnostics if you extend the UI"
  ]
}
  ]
}
